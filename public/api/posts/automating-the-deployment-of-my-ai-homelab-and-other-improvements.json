{
  "slug": "automating-the-deployment-of-my-ai-homelab-and-other-improvements",
  "title": "Automating the deployment of my Homelab AI  Infrastructure",
  "description": "In a previous post, I wrote about using my VMware lab with an NVIDIA Tesla P4 for running some AI services. However, this deployment was done with the GPU in",
  "date": "2026-01-15T21:19:58Z",
  "modified": "2026-02-05T23:36:32Z",
  "author": "James Kilby",
  "url": "https://jameskilby.co.uk/2026/01/automating-the-deployment-of-my-ai-homelab-and-other-improvements/",
  "markdown_url": "/markdown/posts/automating-the-deployment-of-my-ai-homelab-and-other-improvements.md",
  "api_url": "/api/posts/automating-the-deployment-of-my-ai-homelab-and-other-improvements.json",
  "categories": [
    "Ansible",
    "Artificial Intelligence",
    "Containers",
    "Devops",
    "Homelab",
    "NVIDIA",
    "Traefik",
    "VMware",
    "Docker",
    "Hosting",
    "Kubernetes",
    "vSAN",
    "VCF",
    "Cloudflare",
    "Github",
    "Wordpress",
    "Storage",
    "Synology",
    "Personal"
  ],
  "image": "https://jameskilby.co.uk/wp-content/uploads/2026/01/VMware-NVIDIA-logos_ee2f18dc-615d-4c9e-8f11-9c3c2ce2bf37-prv.png",
  "content": "![](https://jameskilby.co.uk/wp-content/uploads/2026/01/VMware-NVIDIA-logos_ee2f18dc-615d-4c9e-8f11-9c3c2ce2bf37-prv.png)\n\n[Ansible](https://jameskilby.co.uk/category/ansible/) | [Artificial Intelligence](https://jameskilby.co.uk/category/artificial-intelligence/) | [Containers](https://jameskilby.co.uk/category/containers/) | [Devops](https://jameskilby.co.uk/category/devops/) | [Homelab](https://jameskilby.co.uk/category/homelab/) | [NVIDIA](https://jameskilby.co.uk/category/nvidia/) | [Traefik](https://jameskilby.co.uk/category/traefik/) | [VMware](https://jameskilby.co.uk/category/vmware/)\n\n# Automating the deployment of my Homelab AI Infrastructure\n\nBy[James](https://jameskilby.co.uk) January 15, 2026February 5, 2026 ‚Ä¢ üìñ8 min read(1,571 words)\n\nüìÖ **Published:** January 15, 2026‚Ä¢ **Updated:** February 05, 2026\n\nIn a previous post, I wrote about using my VMware lab with an NVIDIA Tesla P4 for running some AI services. However, this deployment was done with the GPU in passthrough mode (I will refer to this a GPU). I wanted to take this to the next level and I also wanted to automate most of the steps. This was for a few reasons, firstly I wanted to get better at automating in general. Secondly, I found the setup brittle and wanted to improve the reliability of deployments. This post will be about using automation to deploying of the VM infrastructure required to be able to run AI Workloads. \n\nI also decided to bite the bullet and update the graphics card I was using to something a bit more modern and capable. After a bit of searching I decided on an [Nvidia A10](https://www.nvidia.com/en-gb/data-center/products/a10-gpu/)\n\nHere is my write-up, and a link to my GitHub [repository](https://github.com/jameskilbynet/iac/tree/main/ansible) with the relevant code.\n\n## Table of Contents\n\n## GPU vs vGPU\n\nFor anyone not familiar, it may be worth giving an overview of the differences between GPU and vGPU. What I describe as ‚ÄúGPU‚Äù is where I am passing the entire Graphics PCI device through to a single VM within vSphere. Using this method has some advantages, firstly, it doesn‚Äôt require any special licences or drivers. The entire PCI card gets passed into the VM as is. However, it has some downsides. The two most important ones for me are that it can only be presented to a single VM at a time and that VM cannot be snapshotted while it is turned on. This made backups convoluted. As I was changing configurations a lot this became tedious. I also wanted to be able to pass the card through to multiple VM‚Äôs\n\nvGPU is only officially supported on ‚Äúdatacentre‚Äù cards from NVIDIA it virtualizes the graphics card and allows you to share it across multiple VM‚Äôs in a similar way to what vSphere has done for compute virtualization for years. It allows you to split the card into some predefined profiles that I have listed below and attach them to multiple virtual machines at the same time.\n\n## Pre req‚Äôs\n\nThere are quite a lot of pre-reqs required to be in place to utilise the attached deployment scripts. So it‚Äôs worth ensuring that all of these are complete.\n\n  * Obviously, you need a vSphere Host and vCentre licensed at least at the enterprise level (I am using Enterprise Plus on 8.0u3)\n  * NVIDIA datacenter Graphics card and associated Host and Guest drivers, I am using an A10 and using driver version 535.247.0 \n  * NVIDIA vGPU licence. Trials are available [here](https://www.nvidia.com/en-gb/data-center/resources/vgpu-evaluation/) if needed\n  * NFS Server (used for NVIDIA software deployment)\n  * Domain hosted with Cloudflare and API [token](http://You can create one at: https://dash.cloudflare.com/profile/api-tokens) with Zone:DNS:Edit permissions.\n  * SSH access to the vGPU VM with SUDO permission\n  * Internal DNS Records created for \n    * vGPU VM\n    * Traefik Dashboard\n    * Test NGINX Server ‚Äì Optional\n    * NFS Server (Used for vGPU Software install)\n\n## Host Preparation\n\nTo make vGPU work successfully, there are two elements required. The first is that the vSphere host has the driver installed. For now, I‚Äôm using the ‚Äò535‚Äô version of the driver, which is the LTS version.` To utilise vGPU with vSphere you need an NVIDIA account to obtain the software. Once this has been obtained, you need to copy the host driver to a VMware datastore. Place the host in maintenance mode and then install the driver.\n    \n    \n    esxcli software vib install -d /vmfs/volumes/02cb3d2b-457ccb84/nvidia/NVIDIA-GRID-vSphere-8.0-535.247.02-535.247.01-539.28.zip\n\nüìã Copy\n\nIf it worked successfully, you should get a result like the below\n    \n    \n    Installation Result\n       Message: Operation finished successfully.\n       VIBs Installed: NVD_bootbank_NVD-VMware_ESXi_8.0.0_Driver_535.247.02-1OEM.800.1.0.20613240\n       VIBs Removed: \n       VIBs Skipped: \n       Reboot Required: false\n       DPU Results: \n\nüìã Copy\n\nI always choose to restart the host after installing the driver. I have been bitten in the past where it says a reboot isn‚Äôt required but it was.\n\nOnce the host has restarted ssh into it and validate that the driver is talking to the card correctly with the nvidia-smi command. This is executed on the ESXi host.\n    \n    \n    +---------------------------------------------------------------------------------------+  \n    | NVIDIA-SMI 535.247.02             Driver Version: 535.247.02   CUDA Version: N/A      |  \n    |-----------------------------------------+----------------------+----------------------+  \n    | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |  \n    | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |  \n    |                                         |                      |               MIG M. |  \n    |=========================================+======================+======================|  \n    |   0  NVIDIA A10                     On  | 00000000:03:00.0 Off |                    0 |  \n    |  0%   41C    P8              22W / 150W |  11200MiB / 23028MiB |      0%      Default |  \n    |                                         |                      |                  N/A |  \n    +-----------------------------------------+----------------------+----------------------+  \n      \n    \n\nüìã Copy\n\n## Guest Setup\n\nI have built some Ansible playbooks that perform a number of activities that make it MUCH easier to get the guest up and running for use with AI workloads. These have only been tested with Ubuntu.\n\nAt a high level, they:\n\n  * [Patch Ubuntu ](https://github.com/jameskilbynet/iac/blob/main/ansible/updates/patch_ubuntu.yml)\n  * [Deploy Docker ](https://github.com/jameskilbynet/iac/blob/main/ansible/docker/install_docker.yml)\n  * [Install NVIDIA Guest drivers](https://github.com/jameskilbynet/iac/blob/main/ansible/vGPU/install_nvidia_drivers.yml)\n  * [Install NVIDIA Container Toolkit](http://ansible/vGPU/install_nvidia_containertoolkit.yml)\n  * [Install Traefik ](https://github.com/jameskilbynet/iac/blob/main/ansible/traefik/traefik_deploy.yml)(Optional) but highly recommended\n\n## Ansible Details \n\nI have kept these as separate playbooks for now. This hopefully makes it easier to follow and/or troubleshoot if needed. The playbooks are intended to be run in order.\n\nI am using SemaphoreUI to handle the deployment but this isn‚Äôt required. \n\n### 1\\. Docker deployment\n\nI have covered the deployment of Docker already in [this](https://jameskilby.co.uk/2025/09/managing-my-homelab-with-semaphoreui/ \"Managing my Homelab with SemaphoreUI\") post. Obviously, you don‚Äôt have to use Semaphore to do this (especially as Semaphore requires Docker in the first place). However, you can use the Ansible playbook with some amendments.\n\nNo parameters are required to be set. Everything is set in the playbook.\n\n### 2\\. Install NVIDIA Guest Drivers\n\nThis playbook configures an NFS client on the Ubuntu server and then copies both the vGPU license file and the driver from my NFS storage before installing them.\n\nA number of parameters need to be defined or amended to run this successfully in your environment.\n\nThe following parameters need to be set in SemaphoreUI as a variable group. \n\nJump to the full Semaphore Instructions at the bottom\n\nVariable| Default| Description  \n---|---|---  \n`nvidia_vgpu_driver_file`| `NVIDIA-Linux-x86_64-535.247.01-grid.run`| Driver installer filename  \n`nvidia_vgpu_licence_file`| `client_configuration_token_04-08-2025-16-54-19.tok`| License token filename  \n`nvidia_vgpu_driver_path`| `/tmp/<driver_file>`| Local path for installer  \n`nvidia_nfs_server`| `nas.jameskilby.cloud`| NFS server hostname  \n`nvidia_nfs_export_path`| `/mnt/pool1/ISO/nvidia`| NFS export path  \n`nvidia_nfs_mount_point`| `/mnt/iso/nvidia`| Local mount point  \n  \n### 3\\. Install NVIDIA Container Toolkit\n\nThis playbook installs the NVIDIA container toolkit and validates that everything is working by running a Docker container and executing the nvidia-smi command from within a container.\n\nNo parameters are required to be set\n\n### 4\\. Install Traefik\n\nTraefik is a reverse proxy/ingress controller. I am using it effectively as a load balancer in front of my web-based homelab services. I also have it integrated with Let‚Äôs Encrypt and Cloudflare so that it will automatically obtain a trusted certificate for my internal services. This has the added benefit that I don‚Äôt need to remember the relevant port the containers are running on.\n\nThis playbook needs a lot of Variables as can be seen below. In most cases the default is ok.\n\nWhen they are all input to Semaphore it will look something like this.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/Screenshot-2026-02-05-at-23.01.04-806x1024.png)\n\nRather than typing all of the values out you can copy the json below and then just tweak what you need to.\n    \n    \n    {\n      \"nvidia_vgpu_driver_file\": \"NVIDIA-Linux-x86_64-535.247.01-grid.run\",\n      \"nvidia_vgpu_licence_file\": \"client_configuration_token_04-08-2025-16-54-19.tok\",\n      \"nvidia_vgpu_driver_path\": \"/tmp/<driver_file>\",\n      \"nvidia_nfs_server\": \"nas.jameskilby.cloud\",\n      \"nvidia_nfs_export_path\": \"/mnt/pool1/ISO/nvidia\",\n      \"nvidia_nfs_mount_point\": \"/mnt/iso/nvidia\",\n      \"traefik_deploy_test_service\": \"true\",\n      \"traefik_healthcheck_poll_retries\": \"12\",\n      \"traefik_healthcheck_poll_delay\": \"5\",\n      \"traefik_docker_image\": \"traefik:v3.6.4\",\n      \"traefik_name\": \"traefik\",\n      \"traefik_config_path\": \"/etc/traefik\",\n      \"traefik_acme_path\": \"/etc/traefik/acme\",\n      \"traefik_docker_network\": \"traefik\",\n      \"traefik_http_port\": \"80\",\n      \"traefik_https_port\": \"443\",\n      \"acme_dns_delay\": \"10\",\n      \"acme_dns_resolvers\": \"['1.1.1.1:53', '8.8.8.8:53']\",\n      \"traefik_log_level\": \"info\",\n      \"traefik_log_format\": \"json\",\n      \"traefik_log_max_size\": \"10m\",\n      \"traefik_log_max_files\": \"3\",\n      \"traefik_healthcheck_interval\": \"30s\",\n      \"traefik_healthcheck_timeout\": \"10s\",\n      \"traefik_healthcheck_retries\": \"3\",\n      \"traefik_healthcheck_start_period\": \"30s\",\n      \"traefik_test_container\": \"nginx-test\",\n      \"traefik_test_image\": \"nginx:alpine\",\n      \"traefik_test_domain\": \"test.<wildcard_domain>\",\n      \"traefik_dashboard_user\": \"admin\"\n    }\n\nüìã Copy\n\n## Variable Definitions\n\nVariable| Default| Description  \n---|---|---  \n`traefik_deploy_test_service`| ``true``| Set to `false` to skip test nginx deployment  \n`traefik_healthcheck_poll_retries`| `12`| Number of health check poll attempts  \n`traefik_healthcheck_poll_delay`| `5`| Seconds between health check polls  \n`traefik_docker_image`| `traefik:v3`| Traefik Docker image  \n`traefik_name`| `traefik`| Container name  \n`traefik_config_path`| `/etc/traefik`| Config directory  \n`traefik_acme_path`| `/etc/traefik/acme`| ACME/cert directory  \n`traefik_docker_network`| `traefik`| Docker network name  \n`traefik_http_port`| `80`| HTTP port  \n`traefik_https_port`| `443`| HTTPS port  \n`acme_dns_delay`| `10`| Seconds before DNS check  \n`acme_dns_resolvers`| `['1.1.1.1:53', '8.8.8.8:53']`| DNS resolvers  \n`traefik_log_level`| `INFO`| Log level  \n`traefik_log_format`| `json`| Log format  \n`traefik_log_max_size`| `10m`| Max log size  \n`traefik_log_max_files`| `3`| Max log files  \n`traefik_healthcheck_interval`| `30s`| Health check interval  \n`traefik_healthcheck_timeout`| `10s`| Health check timeout  \n`traefik_healthcheck_retries`| `3`| Health check retries  \n`traefik_healthcheck_start_period`| `30s`| Health check grace period  \n`traefik_test_container`| `nginx-test`| Test container name  \n`traefik_test_image`| `nginx:alpine`| Test container image  \n`traefik_test_domain`| `test.<wildcard_domain>`| Test service domain  \n`traefik_dashboard_user`| `admin`| Dashboard username  \n  \nIt also needs additional variables that should be classed as secrets as they are sensitive\n\nThese are the Traefik admin dashboard hash and the Cloudflare API token\n\n## Semaphore Implementation Instructions\n\nAssuming you are going to use SemaphoreUI to execute the playbooks these are the steps you will need to take. If you haven‚Äôt already set it up review my guide [here](https://jameskilby.co.uk/2025/09/managing-my-homelab-with-semaphoreui/)\n\nThe first step is to define a new repository where the playbooks will be executed from. As my Git repo is public no authentication is required. You also need to specify the branch, in this case I am using main.\n\n### Repository\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/SemaphoreRepo-e1769682413872.png)\n\n### Key Store\n\nYou also need to define the authentication from Semaphore to the target workload. This is done in the Key Store section.\n\nI do this in two parts. The first is auth which I choose to do with SSH key \n\nI then also store the become password in Semaphore for use as SUDO\n\n## Inventory\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/Sempahore-Inventory-665x1024.png)\n\n### Task Templates\n\n## Variable Group\n\nWe will need to create a variable group and set the variables when we come to install the NVIDIA drivers\n\nReview the variable table above and set to match your environment. This is what mine looks like\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/Set_Variable_Group-922x1024.png)\n\nThen ensure that the task template is set to use it\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/Variable-Group-1024x547.png)\n\n## üìö Related Posts\n\n  * [Managing my Homelab with SemaphoreUI](https://jameskilby.co.uk/2025/09/managing-my-homelab-with-semaphoreui/)\n\n## Similar Posts\n\n  * [ ![Use Portainer in a Homelab with GitHub](https://jameskilby.co.uk/wp-content/uploads/2022/12/22225832.png) ](https://jameskilby.co.uk/2022/12/use-portainer-in-a-homelab-with-github/)\n\n[Docker](https://jameskilby.co.uk/category/docker/) | [Homelab](https://jameskilby.co.uk/category/homelab/) | [Hosting](https://jameskilby.co.uk/category/hosting/) | [Kubernetes](https://jameskilby.co.uk/category/kubernetes/)\n\n### [Use Portainer in a Homelab with GitHub](https://jameskilby.co.uk/2022/12/use-portainer-in-a-homelab-with-github/)\n\nBy[James](https://jameskilby.co.uk) December 9, 2022October 1, 2025\n\nLate to the party or not, I have been using containers in my lab more and more and that has led me to Portainer‚Ä¶. I use it for managing the docker containers on my Synology but it can also be used for managing lots of other things. In their own words ‚ÄúPortainer accelerates container adoption‚Ä¶.\n\n  * [ ![vSAN Cluster Shutdown ‚Äì Orchestration](https://jameskilby.co.uk/wp-content/uploads/2023/11/OrigionalPoweredByvSAN-550x324-1.jpg) ](https://jameskilby.co.uk/2025/12/vsan-cluster-shutdown/)\n\n[VMware](https://jameskilby.co.uk/category/vmware/) | [vSAN](https://jameskilby.co.uk/category/vmware/vsan-vmware/)\n\n### [vSAN Cluster Shutdown ‚Äì Orchestration](https://jameskilby.co.uk/2025/12/vsan-cluster-shutdown/)\n\nBy[James](https://jameskilby.co.uk) December 6, 2025February 1, 2026\n\nHow to safety shutdown a vSAN Environment\n\n  * [ ![MultiHost Holodeck VCF](https://jameskilby.co.uk/wp-content/uploads/2023/12/Holodeck-Overview.png) ](https://jameskilby.co.uk/2024/01/multihost-holodeck-vcf/)\n\n[VMware](https://jameskilby.co.uk/category/vmware/) | [VCF](https://jameskilby.co.uk/category/vmware/vcf/)\n\n### [MultiHost Holodeck VCF](https://jameskilby.co.uk/2024/01/multihost-holodeck-vcf/)\n\nBy[James](https://jameskilby.co.uk) January 17, 2024January 18, 2026\n\nHow to Deploy VMware Holodeck on multiple hosts\n\n  * [ ![How I upgraded my blog as a  Static Website with GitHub Actions and Cloudflare](https://jameskilby.co.uk/wp-content/uploads/2025/10/Github-Actions.webp) ](https://jameskilby.co.uk/2025/10/how-i-deploy-my-blog-as-a-static-website-with-github-actions-and-cloudflare/)\n\n[Cloudflare](https://jameskilby.co.uk/category/cloudflare/) | [Devops](https://jameskilby.co.uk/category/devops/) | [Github](https://jameskilby.co.uk/category/github/) | [Wordpress](https://jameskilby.co.uk/category/wordpress/)\n\n### [How I upgraded my blog as a Static Website with GitHub Actions and Cloudflare](https://jameskilby.co.uk/2025/10/how-i-deploy-my-blog-as-a-static-website-with-github-actions-and-cloudflare/)\n\nBy[James](https://jameskilby.co.uk) October 6, 2025February 1, 2026\n\nI wanted to automate the publishing of my blog from the authoring side to the public side. These are some of the improvements I made. What I started with My previous setup, involved a locally hosted WordPress instance. This runs in my homelab in an Ubuntu VM. This I will refer to as the authoring‚Ä¶\n\n  * [Homelab](https://jameskilby.co.uk/category/homelab/) | [Storage](https://jameskilby.co.uk/category/storage/) | [Synology](https://jameskilby.co.uk/category/synology/)\n\n### [Lab Storage](https://jameskilby.co.uk/2018/01/lab-storage/)\n\nBy[James](https://jameskilby.co.uk) January 6, 2018July 10, 2024\n\nI have been meaning to post around some of the lab setup for a while. Although it changes frequently at present it‚Äôs as below. I will add some pics when I have tidied up the lab/cables My primary lab storage is all contained within an HP Gen8 Microserver. Currently Configured: 1x INTEL Core i3-4130 running at‚Ä¶\n\n  * [ ![My First Pull](https://jameskilby.co.uk/wp-content/uploads/2020/12/175jvBleoQfAZJc3sgTSPQA.jpg) ](https://jameskilby.co.uk/2020/12/my-first-pull/)\n\n[Devops](https://jameskilby.co.uk/category/devops/) | [Personal](https://jameskilby.co.uk/category/personal/)\n\n### [My First Pull](https://jameskilby.co.uk/2020/12/my-first-pull/)\n\nBy[James](https://jameskilby.co.uk) December 22, 2020December 8, 2025\n\nI was initially going to add in the contents of this post to one that I have been writing about my exploits with HashiCorp Packer but I decided it probably warranted being separated out. While working with the following awesome project I noticed a couple of minor errors and Improvements that I wanted to suggest‚Ä¶.",
  "excerpt": "![](https://jameskilby.co.uk/wp-content/uploads/2026/01/VMware-NVIDIA-logos_ee2f18dc-615d-4c9e-8f11-9c3c2ce2bf37-prv.png)\n\n[Ansible](https://jameskilby.co.uk/category/ansible/) | [Artificial Intelligence](https://jameskilby.co.uk/category/artificial-intelligence/) | [Containers](https://jameskilby.c..."
}