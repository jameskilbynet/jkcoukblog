{
  "slug": "automating-the-deployment-of-my-ai-homelab-and-other-improvements",
  "title": "Automating the deployment of my Homelab AI  Infrastructure",
  "description": "How I use Ansible automation to deploy my AI Infrastucture",
  "date": "2026-02-09T11:54:54Z",
  "modified": "2026-02-09T11:54:55Z",
  "author": "James Kilby",
  "url": "https://jameskilby.co.uk/2026/02/automating-the-deployment-of-my-ai-homelab-and-other-improvements/",
  "markdown_url": "/markdown/posts/automating-the-deployment-of-my-ai-homelab-and-other-improvements.md",
  "api_url": "/api/posts/automating-the-deployment-of-my-ai-homelab-and-other-improvements.json",
  "categories": [
    "Ansible",
    "Artificial Intelligence",
    "Containers",
    "Devops",
    "Homelab",
    "NVIDIA",
    "Traefik",
    "VMware",
    "Personal",
    "Nutanix",
    "Storage",
    "VMware Cloud on AWS",
    "Veeam"
  ],
  "tags": [
    null,
    null,
    null,
    null,
    null
  ],
  "image": "https://jameskilby.co.uk/wp-content/uploads/2026/01/VMware-NVIDIA-logos_ee2f18dc-615d-4c9e-8f11-9c3c2ce2bf37-prv.png",
  "content": "![](https://jameskilby.co.uk/wp-content/uploads/2026/01/VMware-NVIDIA-logos_ee2f18dc-615d-4c9e-8f11-9c3c2ce2bf37-prv.png)\n\n[Ansible](https://jameskilby.co.uk/category/ansible/) | [Artificial Intelligence](https://jameskilby.co.uk/category/artificial-intelligence/) | [Containers](https://jameskilby.co.uk/category/containers/) | [Devops](https://jameskilby.co.uk/category/devops/) | [Homelab](https://jameskilby.co.uk/category/homelab/) | [NVIDIA](https://jameskilby.co.uk/category/nvidia/) | [Traefik](https://jameskilby.co.uk/category/traefik/) | [VMware](https://jameskilby.co.uk/category/vmware/)\n\n# Automating the deployment of my Homelab AI Infrastructure\n\nBy[James](https://jameskilby.co.uk) February 9, 2026February 9, 2026 ‚Ä¢ üìñ17 min read(3,360 words)\n\nIn a previous [post](https://jameskilby.co.uk/2024/10/self-hosting-ai-stack-using-vsphere-docker-and-nvidia-gpu/), I wrote about using my VMware lab with an NVIDIA Tesla P4 for running some AI services. This deployment was done with the P4 GPU in passthrough mode where the entire PCI card was presented into the VM. (I will refer to this as GPU mode). I wanted to take this to the next level. I also wanted to automate most of the steps. This was for a few reasons; firstly, I wanted to get better at automation in general. Secondly, I found the setup brittle and wanted to improve the reliability of deployments. This post will be about using automation to deploy the VM infrastructure required to be able to run AI Workloads. This is something I presented on with my good friend [Gareth](https://www.virtualisedfruit.co.uk/) at the London VMUG. Check out the recording of that [here.](https://youtu.be/Dt6m9JdsrIM) Like there, we both discussed how there are quite a few layers to getting the infrastructure right and doing it in an Enterprise level is tricky. Fundamentally, that‚Äôs why products like VMware Private AI Foundation [exis](https://www.vmware.com/solutions/cloud-infrastructure/private-ai)t.\n\nHowever, in a homelab enviroment a more straightforward Docker-based setup could be more appropriate‚Ä¶.\n\nI also decided to bite the bullet and update the graphics card I was using to something a bit more modern and capable. After a bit of searching, I decided on an [NVIDIA A10](https://www.nvidia.com/en-gb/data-center/products/a10-gpu/)\n\nThis post is about getting the foundational infrastructure ready. The first part will explain the steps and detail the Ansible configuration. The second part of this post goes into a full deployment utilising [SemaphoreUI](https://jameskilby.co.uk/2025/09/managing-my-homelab-with-semaphoreui/). My next post will go into more details on the Docker containers I am using for actually running the AI services, building on these foundations.\n\nAll of the code is referenced in my GitHub IAC [repository](https://github.com/jameskilbynet/iac/tree/main/ansible)\n\n## Table of Contents\n\n## GPU vs vGPU\n\nFor anyone not familiar, it may be worth giving an overview of the differences between GPU and vGPU. What I describe as ‚ÄúGPU‚Äù is where I am passing the entire Graphics PCI device through to a single VM within vSphere. Using this method has some advantages; firstly, it doesn‚Äôt require any special licences or drivers. The entire PCI card gets passed into the VM as is. However, it has some downsides. The two most important ones for me are that it can only be presented to a single VM at a time and that VM cannot be snapshotted while it is turned on. This made backups convoluted. As I was changing configurations a lot this became tedious. I also wanted to be able to pass the card through to multiple VM‚Äôs\n\nvGPU is only officially supported on ‚Äúdatacentre‚Äù cards from NVIDIA. It virtualises the graphics card and allows you to share it across multiple VMs in a similar way to what vSphere has done for compute virtualisation for years. It allows you to split the card into some predefined profiles that I have listed below and attach them to multiple virtual machines at the same time.\n\n## Pre req‚Äôs\n\nThere are quite a lot of pre-reqs required to be in place to utilise all the attached deployment playbooks. So it‚Äôs worth ensuring that all of these are complete.\n\n  * Obviously, you need a vSphere Host and vCentre server licensed at least at the enterprise level (I am using Enterprise Plus on 8.0u3)\n  * An Ubuntu VM (Tested with 25.04) referred to as the vGPU VM\n  * A NVIDIA datacenter Graphics card and associated Host and Guest drivers, I am using an A10 24GB and using driver version 535.247.0\n  * NVIDIA vGPU licence. Trials are available [here](https://www.nvidia.com/en-gb/data-center/resources/vgpu-evaluation/) if needed\n  * NFS Server (used for NVIDIA software deployment)\n  * A Domain hosted with Cloudflare and an API [token](http://You can create one at: https://dash.cloudflare.com/profile/api-tokens) with Zone:DNS:Edit permissions.\n  * SSH access to the vGPU VM with SUDO permission\n  * Somewhere to run Ansible playbooks from.\n  * Internal DNS Records created for \n    * vGPU VM\n    * Traefik Dashboard\n    * Test NGINX Server ‚Äì Optional\n    * NFS Server (Used for vGPU Software install)\n\n## Host Setup\n\nTo make vGPU work successfully, there are three elements required. The ESXi Driver/VIB, the Guest driver and the NVIDIA licence for the Guest VM. For now, I‚Äôm using the ‚Äò535‚Äô version of the driver bundle, which is the LTS version.` The Driver bundle and licence must be obtained from NVIDIA however trials are available if you don‚Äôt have access. Once the software has been obtained, you need to copy the host driver to a VMware datastore. Then place the host in maintenance mode and install the driver using the command.\n    \n    \n    esxcli software vib install -d /vmfs/volumes/02cb3d2b-457ccb84/nvidia/NVIDIA-GRID-vSphere-8.0-535.247.02-535.247.01-539.28.zip\n\nüìã Copy\n\nIf it worked successfully, you should get a result like the one below\n    \n    \n    Installation Result\n       Message: Operation finished successfully.\n       VIBs Installed: NVD_bootbank_NVD-VMware_ESXi_8.0.0_Driver_535.247.02-1OEM.800.1.0.20613240\n       VIBs Removed: \n       VIBs Skipped: \n       Reboot Required: false\n       DPU Results: \n\nüìã Copy\n\nI always choose to restart the host after installing the driver. I have been bitten in the past where it says a reboot isn‚Äôt required, but it was.\n\nOnce the host has restarted, ssh into it and validate that the driver is talking to the GPU correctly. This can be done with the nvidia-smi command. This is executed on the ESXi host and will show something similar to the below if it‚Äôs working.\n    \n    \n    +---------------------------------------------------------------------------------------+\n    | NVIDIA-SMI 535.247.02             Driver Version: 535.247.02   CUDA Version: N/A      |\n    |-----------------------------------------+----------------------+----------------------+\n    | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n    | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n    |                                         |                      |               MIG M. |\n    |=========================================+======================+======================|\n    |   0  NVIDIA A10                     On  | 00000000:03:00.0 Off |                    0 |\n    |  0%   41C    P8              22W / 150W |  11200MiB / 23028MiB |      0%      Default |\n    |                                         |                      |                  N/A |\n    +-----------------------------------------+----------------------+----------------------+\n\nüìã Copy\n\n### vGPU Profiles\n\nIf everything is working correctly, you should now be able to see the vGPU profiles in vCentre. Select the VM you want to present the NVIDIA card to. Edit the VM settings and select add PCI device.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/Add-PCI-Device-1-402x1024.png)\n\nYou should then be presented with the available profiles from the NVIDIA GPU\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/GPU-Profiles-1024x397.png)\n\nAs can be seen, the NVIDIA A10 supports 18 different profiles. The nvidia_a10 part specifies the card (in case you have multiple), and the suffix determines how much vGPU RAM is allocated to the vGPU instance, along with the features that are exposed with it. I typically use either the a10-24q profile to allow the use of larger AI models. Although I do step this down to the 12q profile to allow support of multiple vm‚Äôs for testing etc.\n\nThe NVIDIA A10 supports four separate ‚ÄúPersonalities) A, B, Q and C each of these come with different capabilities but also potentially different licensing requirements.\n\nProfile| Intended Use| Driver Feature Set| License Tier| Expected Workloads  \n---|---|---|---|---  \nA-Series| App Streaming / RDSH| Basic Graphics no pro features| vAPPS| Many Users / Published Apps  \nB-Series| VDI Desktops| Standard Desktop Graphics| vPC| Knowledge Worker Desktops  \nC-Series| Compute Only| CUDA Only | vCompute Server| AI/ML Data Science  \nQ-Series| Full GFX Workstation| Full RTX| RTX vWS| CAD,3D Rendering   \n  \nThe keen-eyed amongst you might realise that I am using the wrong profile for AI workloads. I have chosen to use Q-based profiles rather than C-based ones. This is because the A, B and Q profiles are run from one ESXi driver VIB and the C profile requires a different VIB. Because I occasionally want to run a VDI based workload or media conversion in my lab. I have chosen to use the more graphics focused VIB and utilise the Q profile for my AI based workloads. I suspect this might have a performance impact, but it was a compromise I was willing to accept.\n\n## Guest Setup\n\nTo make the guest setup easier and more repeatable, I have built some Ansible playbooks. These perform a number of activities that make it MUCH easier to get the guest up and running for use with AI workloads and other containers. These have only been tested with Ubuntu but will probably work with other Linux distro‚Äôs without a lot of changes. As a bonus if you want to utilise the Traefik setup I have built this can be used on its own, just with the Deploy Docker and Install Traefik playbooks. Once these playbooks have been run, you will be in a position to spin up any number of Docker containers with GPU access.\n\nAt a high level, the four playbooks I have built:\n\n  * [Deploy Docker ](https://github.com/jameskilbynet/iac/blob/main/ansible/docker/install_docker.yml)\n  * [Install NVIDIA Guest drivers](https://github.com/jameskilbynet/iac/blob/main/ansible/vGPU/install_nvidia_drivers.yml)\n  * [Install NVIDIA Container Toolkit](http://ansible/vGPU/install_nvidia_containertoolkit.yml)\n  * [Install Traefik ](https://github.com/jameskilbynet/iac/blob/main/ansible/traefik/traefik_deploy.yml)\n\n### Guest Configuration\n\nBefore we get to the Ansible section, we obviously need to build the VM. There are a few key steps here as well. Firstly, it must be an EFI-based VM; BIOS won‚Äôt work. Secondly, you need to allocate a decent amount of RAM to the workload. I usually run with at least 3x my GPU vRAM. Therefore, my main AI VM usually has 96GB allocated to it. It is also important to understand that vSphere will automatically reserve all of this memory. Also, make sure that you are not swapping to disk. This will dramatically slow down the system.\n\nThe next step is to ensure that the VM has access to a lot of fast storage. I would recommend at least a few hundred GB, especially if you are going to be working with multiple models. The smallest models I typically use are around 8GB. Therefore loading these into memory even from an NVMe drive, can take a few seconds before anything can be processed. This will always give a slight delay when cold-starting compared to a commercial equivalent (Think ChatGPT, etc.) It is possible to configure Ollama to keep the models loaded. I have set this to 1 hour before it unloads them. This is due to higher power consumption on the graphics card when they are loaded.\n\nPlease note that all of the testing has been done on Ubuntu 24 or 25\n\n### Ansible Details \n\nI have kept these as separate playbooks for now. This hopefully makes it easier to follow and/or troubleshoot if needed. The playbooks are intended to be run in order. \n\nI am using SemaphoreUI to handle the Ansible deployment but this isn‚Äôt required. If you are familiar enough with Ansible to not use Semaphore, then you can easily modify these to suit your execution preferences.\n\n#### 1\\. Docker deployment\n\n<https://github.com/jameskilbynet/iac/blob/main/ansible/docker/install_docker.yml>\n\nI have covered the deployment of Docker already in [this](https://jameskilby.co.uk/2025/09/managing-my-homelab-with-semaphoreui/ \"Managing my Homelab with SemaphoreUI\") post. No parameters are required to be set for the Docker deployment. Everything is set in the playbook.\n\n#### 2\\. Install NVIDIA Guest Drivers\n\n<https://github.com/jameskilbynet/iac/blob/main/ansible/vGPU/install_nvidia_drivers.yml>\n\nThis playbook configures an NFS client on the Ubuntu server and then copies both the vGPU license file and the driver from my NFS storage before installing them. This, I felt was the best way to have a portable Ansible file without checking in my NVIDIA licence and driver files into Git.\n\nBefore you use this playbook several variables need to be set in Semaphore. These should be done in the Variable Group section. I created a Variable Group called vGPU for this purpose.\n\n##### NVIDIA Guest Driver Variables\n\n**Variable**|  **Default**|  **Description**  \n---|---|---  \nnvidia_vgpu_driver_file| NVIDIA-Linux-x86_64-535.247.01-grid.run| Driver filename  \nnvidia_vgpu_licence_file| client_configuration_token_04-08-2025-16-54-19.tok| Token filename  \nnvidia_nfs_server| nas.jameskilby.cloud| NFS server hostname  \nnvidia_nfs_export_path| /mnt/pool1/ISO/nvidia| NFS export path  \nnvidia_nfs_mount_point| /mnt/iso/nvidia| Local mount point  \n  \nTo make your life easier, you can copy this json to Semaphore and just tweak what is needed.\n    \n    \n    {\n      \"nvidia_vgpu_driver_file\": \"NVIDIA-Linux-x86_64-535.247.01-grid.run\",\n      \"nvidia_vgpu_licence_file\": \"client_configuration_token_04-08-2025-16-54-19.tok\",\n      \"nvidia_nfs_server\": \"nas.jameskilby.cloud\",\n      \"nvidia_nfs_export_path\": \"/mnt/pool1/ISO/nvidia\",\n      \"nvidia_nfs_mount_point\": \"/mnt/iso/nvidia\",\n    }\n\nüìã Copy\n\nIf everything runs smoothly the last step of the Ansible playbook will show an export of the licence and vGPU configuration.\n    \n    \n    TASK [Show vGPU license info] **************************************************\n    9:18:47 PM\n    ok: [blogtest.jameskilby.cloud] => \n    9:18:47 PM\n      msg: |-\n    9:18:47 PM\n        ==============NVSMI LOG==============\n    9:18:47 PM\n      \n    9:18:47 PM\n        Timestamp                                 : Fri Feb  6 21:18:47 2026\n    9:18:47 PM\n        Driver Version                            : 535.247.01\n    9:18:47 PM\n        CUDA Version                              : 12.2\n    9:18:47 PM\n      \n    9:18:47 PM\n        Attached GPUs                             : 1\n    9:18:47 PM\n        GPU 00000000:03:00.0\n    9:18:47 PM\n            Product Name                          : NVIDIA A10-12Q\n    9:18:47 PM\n            Product Brand                         : NVIDIA RTX Virtual Workstation\n    9:18:47 PM\n            Product Architecture                  : Ampere\n    9:18:47 PM\n            Display Mode                          : Enabled\n    9:18:47 PM\n            Display Active                        : Disabled\n    9:18:47 PM\n            Persistence Mode                      : Enabled\n    9:18:47 PM\n            Addressing Mode                       : None\n    9:18:47 PM\n            MIG Mode\n    9:18:47 PM\n                Current                           : Disabled\n    9:18:47 PM\n                Pending                           : Disabled\n    9:18:47 PM\n            Accounting Mode                       : Disabled\n    9:18:47 PM\n            Accounting Mode Buffer Size           : 4000\n\nüìã Copy\n\n#### 3\\. Install NVIDIA Container Toolkit\n\n<https://github.com/jameskilbynet/iac/blob/main/ansible/vGPU/install_nvidia_containertoolkit.yml>\n\nThis playbook installs the NVIDIA container toolkit and validates it. It does this by running a Docker container and executing the nvidia-smi command from within the container. This sounds trivial but actually is one of the main reasons I made these playbooks. To get the GPU to work in the docker container you have to have a lot of things set up correctly. This requires the correct ESXi Driver, VM driver within the Ubuntu VM, Docker and the Docker container toolkit set up correctly. The correct Kernel extensions, etc., and the licensing are working correctly\n\nNo additional parameters need to be set to execute it. \n\nBelow is the expected output showcasing nvidia-smi running from within a Docker container.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/nvidia-smi-1024x654.png)\n\n#### 4\\. Install Traefik\n\n<https://github.com/jameskilbynet/iac/blob/main/ansible/traefik/traefik_deploy.yml>\n\nFor those unfamiliar with [Traefik](https://traefik.io) ( It‚Äôs pronounced Traffic), it is a reverse proxy/ingress controller with automatic integrations to Docker and Kubernetes. I have it configured to leverage both Let‚Äôs Encrypt and Cloudflare so that it will automatically obtain signed certificates for me. This is done with Docker labels in such a way that any container I spin up that has the correct Traefik labels will automatically mint a signed certificate and add it to the load balancer. This has the added benefit that I just need to remember a URL IE openwebui.jameskilby.cloud configured in my internal DNS, rather than which VM and port the service is running on. It has the added benefit that it makes it very easy for me to expose services externally if needed. I just add the appropriate DNS record on my external DNS.\n\nThe labels look something like the following. These are added to the individual Docker containers and Traefik can see them through the Docker API. When the containers are started, it will autoconfigure the certificate generation and automatically configure the load balancer. If the container is removed or stopped, it will tidy up the configuration.\n    \n    \n      labels:\n          - \"traefik.enable=true\"\n          - \"traefik.http.routers.open-webui.rule=Host(`openwebui.jameskilby.cloud`)\"\n          - \"traefik.http.routers.open-webui.entrypoints=websecure\"\n          - \"traefik.http.routers.open-webui.tls=true\"\n          - \"traefik.http.routers.open-webui.tls.certresolver=cloudflare\"\n          - \"traefik.http.services.open-webui.loadbalancer.server.port=8080\"\n\nüìã Copy\n\nWith the above labels added to my OpenWebUI container, Traefik will request a certificate for openwebui.jameskilby.cloud and deploy this as a HTTPS service mapped to the OpenWebUI container on port 8080\n\nI could probably do a whole post just on my setup of Traefik but for now here is an overview diagram.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2025/01/Traefik-Docker-Setup-2-1024x342.png)\n\nAs part of the included playbook, I have added the option to deploy a simple nginx web server using this principle. If everything is configured correctly, you should be able to connect to this container post deployment and it will have a fully trusted certificate as seen below. (You may need to wait 1-2 minutes or so after deployment for this process to complete)\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/nginx-cert-1024x997.png)\n\n#### Traefik Dashboard\n\nTraefik also has a nice dashboard that is very useful in troubleshooting.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/TraefikDashboard-1024x478.png)\n\nThis is the dashboard from my testing configuration. My main server has over 40 ‚ÄúRouters‚Äù\n\n#### Traefik Playbook Setup \n\nBefore running this playbook a lot of variables need to be configured as can be seen below. In most cases, the default is OK. I have just extended the vGPU Variable group to do this. When they are all input into Semaphore, it will look something like this.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/Traefik-Variables-804x1024.png)\n\nRather than typing all of the values out you can copy the JSON below and then just tweak what you need to.\n    \n    \n    {\n      \"nvidia_vgpu_driver_file\": \"NVIDIA-Linux-x86_64-535.247.01-grid.run\",\n      \"nvidia_vgpu_licence_file\": \"client_configuration_token_04-08-2025-16-54-19.tok\",\n      \"nvidia_nfs_server\": \"nas.jameskilby.cloud\",\n      \"nvidia_nfs_export_path\": \"/mnt/pool1/ISO/nvidia\",\n      \"nvidia_nfs_mount_point\": \"/mnt/iso/nvidia\",\n      \"traefik_deploy_test_service\": \"true\",\n      \"traefik_healthcheck_poll_retries\": \"12\",\n      \"traefik_healthcheck_poll_delay\": \"5\",\n      \"traefik_docker_image\": \"traefik:v3.6.4\",\n      \"traefik_name\": \"traefik\",\n      \"traefik_config_path\": \"/etc/traefik\",\n      \"traefik_acme_path\": \"/etc/traefik/acme\",\n      \"traefik_docker_network\": \"traefik\",\n      \"traefik_http_port\": \"80\",\n      \"traefik_https_port\": \"443\",\n      \"acme_dns_delay\": \"10\",\n      \"acme_dns_resolvers\": \"[\\\"1.1.1.1:53\\\", \\\"8.8.8.8:53\\\"]\",\n      \"traefik_log_level\": \"info\",\n      \"traefik_log_format\": \"json\",\n      \"traefik_log_max_size\": \"10m\",\n      \"traefik_log_max_files\": \"3\",\n      \"traefik_healthcheck_interval\": \"30s\",\n      \"traefik_healthcheck_timeout\": \"10s\",\n      \"traefik_healthcheck_retries\": \"3\",\n      \"traefik_healthcheck_start_period\": \"30s\",\n      \"traefik_test_container\": \"nginx-test\",\n      \"traefik_test_image\": \"nginx:alpine\",\n      \"traefik_test_domain\": \"test.jameskilby.cloud\",\n      \"traefik_dashboard_user\": \"admin\",\n      \"wildcard_domain\": \"jameskilby.cloud\"\n    }\n\nüìã Copy\n\n##### Traefik Variable Definitions\n\n**Variable**|  **Default**|  **Description**  \n---|---|---  \ntraefik_deploy_test_service| true| set to false to skip NGINX deployment  \ntraefik_healthcheck_poll_retries| 12| Number of health check poll attempts  \ntraefik_healthcheck_poll_delay| 5| Seconds between health check polls  \ntraefik_docker_image| 3.6.4| Traefik Docker Image and version  \ntraefik_name| traefik| Traefik Container Name  \ntraefik_config_path| /etc/traefik| Config Directory  \ntraefik_acme_path| /etc/traefik/acme| ACME/Cert Directory  \ntraefik_docker_network| traefik| Docker Network Name for Traefik  \ntraefik_http_port| 80| HTTP Port  \ntraefik_https_port| 443| HTTPS Port  \nacme_dns_delay| 10| Seconds before DNS Check  \nacme_dns_resolvers| 1.1.1.1:53, 8.8.8.8:53| DNS Resolvers  \ntraefik_log_level| info| Log Level  \ntraefik_log_format| json| Log Format  \ntraefik_log_max_size| 10m| Max Log Size  \ntraefik_log_max_files| 3| Max Log Files  \ntraefik_healthcheck_interval| 30s| Health Check Interval  \ntraefik_heakthcheck_timeout| 10s| Health Check Timeout  \ntraefik_healthcheck_retries| 3| Health Check Retries  \ntraefik_heathcheck_start_period| 30s| Health Check Grace Period  \ntraefik_test_container| nginx-test| Test Container Name  \ntraefik_test_image| nginx:alpine| Test Container Image  \ntraefik_test_domain| test.jameskilby.cloud| Test Service Domain  \ntraefik_dashboard_user| admin| Traefik Dashboard Username  \nwildcard_domain| jameskilby.cloud| Traefik Wildcard Domain   \n  \n#### Ansible Secrets\n\nThe playbook also needs two values that should be considered sensitive. These can be added to Semaphore as secrets. The two secrets are the Cloudflare API Token and the second is a hash of a password for the admin account used to access the Traefik dashboard. The Traefik dashboard is very useful for troubleshooting any SSL/connectivity issues.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/SemaphoreSecrets-1024x262.png)\n\nThe Cloudflare Token needs to be generated in the Cloudflare dashboard. \n\nTo generate the hash for the admin dashboard password, the easiest way is with Docker. The below is an example that can be run on the command line \n    \n    \n    docker run --rm httpd:2   htpasswd -nbB '' 'your_password_here'\n\nüìã Copy\n\nIt will download and execute the httpd container. The last line is the hash that you need.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/httpd-1024x383.png)\n\nIn this case, the hash is: $2y$05$nkQGI2UxnRY.7O.6k14naOJPjslbqOT5vpqZPmXMu4knhBOH1EUAq take the output from this output and add it to the Semaphore secure variable for ‚Äúdashboard_admin_password_hash‚Äù\n\n## Semaphore End-to-End Setup\n\nAssuming you are going to use SemaphoreUI to execute the playbooks, these are the steps you will need to take. If you haven‚Äôt already set it up, review my guide [here](https://jameskilby.co.uk/2025/09/managing-my-homelab-with-semaphoreui/)\n\n### Repository\n\nThe first step is to point Semaphore at the Git repository. This is the location where the playbooks will be executed from. As my Git repo is public no authentication is required. You also need to specify the branch; in this case, I am using main.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/Semaphore_repo.png)\n\n### Key Store\n\nThe next step is to add the Key Store items. This is used to define the authentication from Semaphore to the target workload or other systems. I do this in two parts. The first is standard authentication, which I do with an SSH key. The second part is defining the become password to allow Semaphore to execute SUDO commands. I do this with a password stored in the Key Store. I have called these two methods KeyAuth and PassAuth\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/NewKey-802x1024.png)\n\nOnce the Key authentication is added, the next step is to add a different key store item for when the playbook needs to ‚ÄúBecome‚Äù\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/PassAuth.png)\n\nAn Alternative approach is to allow your user to elevate without confirmation.\n\n### Inventory\n\nOnce the authentication methods are defined, the next step is to update the Semaphore Inventory.\n\nI have created a new Inventory item called ‚ÄúvGPU‚Äô set the User credentials to be KeyAuth and the Sudo credentials as PassAuth as created above.\n\nI‚Äôve then added the specific VM in the inventory. For testing, I have called this blogtest.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/blogtest_ansible_inventory-670x1024.png)\n\n### Variable Group\n\nThe next step is to create a variable group and the required variables. \n\nReview the variable table above and set it to match your environment. This is what mine looks like:\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/VariableGroup-813x1024.png)\n\n### Task Templates\n\nThe final step is to create the four task templates. These are the actual actions that Ansible will perform using the Inventory, Repository and Variable Groups we have just defined.\n\nSelect New Template from the ‚ÄúTask Templates‚Äù view. Ensure it is set to Ansible and configure as per below.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/Variable-Group-1024x547.png)\n\nYou should end up with something looking like this\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/TaskTemplates-1024x263.png)\n\nWhen they are all configured, press the play button to execute each of the given tasks against your inventory item. Semaphore will check out the Ansible playbook and execute it, showing the results in the window.\n\n## Summary\n\nWhen all the playbooks are run successfully, you should now have:\n\n  * A vSphere host capable of passing through NVIDIA datacenter graphics\n  * An Ubuntu VM with vGPU integrated with Docker\n\nThis is now ready for you to start deploying Docker-based AI workloads onto.\n\n## üìö Related Posts\n\n  * [Managing my Homelab with SemaphoreUI](https://jameskilby.co.uk/2025/09/managing-my-homelab-with-semaphoreui/)\n\n## Similar Posts\n\n  * [ ![My First Pull](https://jameskilby.co.uk/wp-content/uploads/2020/12/175jvBleoQfAZJc3sgTSPQA.jpg) ](https://jameskilby.co.uk/2020/12/my-first-pull/)\n\n[Devops](https://jameskilby.co.uk/category/devops/) | [Personal](https://jameskilby.co.uk/category/personal/)\n\n### [My First Pull](https://jameskilby.co.uk/2020/12/my-first-pull/)\n\nBy[James](https://jameskilby.co.uk) December 22, 2020December 8, 2025\n\nI was initially going to add in the contents of this post to one that I have been writing about my exploits with HashiCorp Packer but I decided it probably warranted being separated out. While working with the following awesome project I noticed a couple of minor errors and Improvements that I wanted to suggest‚Ä¶.\n\n  * [ ![VMware ‚Äì Going out with a Bang!](https://jameskilby.co.uk/wp-content/uploads/2023/10/rnli-logo-768x384.png) ](https://jameskilby.co.uk/2023/10/going-out-with-a-bang/)\n\n[VMware](https://jameskilby.co.uk/category/vmware/) | [Personal](https://jameskilby.co.uk/category/personal/)\n\n### [VMware ‚Äì Going out with a Bang!](https://jameskilby.co.uk/2023/10/going-out-with-a-bang/)\n\nBy[James](https://jameskilby.co.uk) October 7, 2023November 17, 2023\n\nThere is a lot of uncertainty with VMware at the moment. This is all due to the pending acquisition by Broadcom. There are a lot of unknowns for the staff and customers about what the company will look like in the future. I certainly have some concerns mainly just with the unknown. However, VMware has‚Ä¶\n\n  * [ ![New Nodes](https://jameskilby.co.uk/wp-content/uploads/2024/07/IMG_6629-768x149.jpeg) ](https://jameskilby.co.uk/2024/07/new-nodes/)\n\n[Homelab](https://jameskilby.co.uk/category/homelab/) | [Nutanix](https://jameskilby.co.uk/category/nutanix/) | [VMware](https://jameskilby.co.uk/category/vmware/)\n\n### [New Nodes](https://jameskilby.co.uk/2024/07/new-nodes/)\n\nBy[James](https://jameskilby.co.uk) July 2, 2024January 18, 2026\n\nI recently decided to update some of my homelab hosts and I managed to do this at very little cost by offloading 2 of my Supermicro e200‚Äôs to fellow vExpert Paul. The below post describes what I bought why and how I have configured it. Table of Contents Node Choice Bill of Materials Rescue IPMI‚Ä¶\n\n  * [ ![TrueNAS Logo](https://jameskilby.co.uk/wp-content/uploads/2023/05/Screenshot-2023-05-22-at-18.49.21-768x198.png) ](https://jameskilby.co.uk/2023/05/homelab-storage-refresh-part-1/)\n\n[Homelab](https://jameskilby.co.uk/category/homelab/) | [Storage](https://jameskilby.co.uk/category/storage/)\n\n### [Homelab Storage Refresh (Part 1)](https://jameskilby.co.uk/2023/05/homelab-storage-refresh-part-1/)\n\nBy[James](https://jameskilby.co.uk) May 23, 2023October 1, 2025\n\nTable of Contents Background ZFS Overview Read Cache (ARC and L2ARC) ZIL (ZFS Intent Log) Hardware Background I have just completed the move of all my production and media-based storage/services to TrueNAS Scale. ( I will just refer to this as TrueNAS) This is based on my HP Z840 and I have now retired my‚Ä¶\n\n  * [ ![VMC Host Errors](https://jameskilby.co.uk/wp-content/uploads/2022/11/iu-1-768x395.png) ](https://jameskilby.co.uk/2020/09/vmc-host-errors/)\n\n[VMware](https://jameskilby.co.uk/category/vmware/) | [VMware Cloud on AWS](https://jameskilby.co.uk/category/vmware/vmware-cloud-on-aws/)\n\n### [VMC Host Errors](https://jameskilby.co.uk/2020/09/vmc-host-errors/)\n\nBy[James](https://jameskilby.co.uk) September 15, 2020February 9, 2026\n\nLean how host failures are handled within VMC\n\n  * [Homelab](https://jameskilby.co.uk/category/homelab/) | [Veeam](https://jameskilby.co.uk/category/veeam/) | [VMware](https://jameskilby.co.uk/category/vmware/)\n\n### [Lab Update ‚Äì Desired Workloads](https://jameskilby.co.uk/2022/01/lab-update-part-5-desired-workloads/)\n\nBy[James](https://jameskilby.co.uk) January 6, 2022November 11, 2023\n\nMy lab is always undergoing change. Partially as I want to try new things or new ways of doing things. Sometimes because I break things (not always by accident) sometimes it‚Äôs a great way to learn‚Ä¶. I decided to list the workloads I am looking to run (some of these are already in place) Infrastucture‚Ä¶",
  "excerpt": "![](https://jameskilby.co.uk/wp-content/uploads/2026/01/VMware-NVIDIA-logos_ee2f18dc-615d-4c9e-8f11-9c3c2ce2bf37-prv.png)\n\n[Ansible](https://jameskilby.co.uk/category/ansible/) | [Artificial Intelligence](https://jameskilby.co.uk/category/artificial-intelligence/) | [Containers](https://jameskilby.c..."
}