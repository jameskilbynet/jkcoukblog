{
  "slug": "automating-the-deployment-of-my-ai-homelab-and-other-improvements",
  "title": "Automating the deployment of my Homelab AI  Infrastructure",
  "description": "In a previous post, I wrote about using my VMware lab with an NVIDIA Tesla P4 for running some AI services. However, this deployment was done with the GPU in",
  "date": "2026-01-15T21:19:58Z",
  "modified": "2026-02-05T23:57:43Z",
  "author": "James Kilby",
  "url": "https://jameskilby.co.uk/2026/01/automating-the-deployment-of-my-ai-homelab-and-other-improvements/",
  "markdown_url": "/markdown/posts/automating-the-deployment-of-my-ai-homelab-and-other-improvements.md",
  "api_url": "/api/posts/automating-the-deployment-of-my-ai-homelab-and-other-improvements.json",
  "categories": [
    "Ansible",
    "Artificial Intelligence",
    "Containers",
    "Devops",
    "Homelab",
    "NVIDIA",
    "Traefik",
    "VMware",
    "Storage",
    "Networking",
    "Veeam",
    "vExpert"
  ],
  "tags": [
    null,
    null,
    null,
    null,
    null
  ],
  "image": "https://jameskilby.co.uk/wp-content/uploads/2026/01/VMware-NVIDIA-logos_ee2f18dc-615d-4c9e-8f11-9c3c2ce2bf37-prv.png",
  "content": "![](https://jameskilby.co.uk/wp-content/uploads/2026/01/VMware-NVIDIA-logos_ee2f18dc-615d-4c9e-8f11-9c3c2ce2bf37-prv.png)\n\n[Ansible](https://jameskilby.co.uk/category/ansible/) | [Artificial Intelligence](https://jameskilby.co.uk/category/artificial-intelligence/) | [Containers](https://jameskilby.co.uk/category/containers/) | [Devops](https://jameskilby.co.uk/category/devops/) | [Homelab](https://jameskilby.co.uk/category/homelab/) | [NVIDIA](https://jameskilby.co.uk/category/nvidia/) | [Traefik](https://jameskilby.co.uk/category/traefik/) | [VMware](https://jameskilby.co.uk/category/vmware/)\n\n# Automating the deployment of my Homelab AI Infrastructure\n\nBy[James](https://jameskilby.co.uk) January 15, 2026February 5, 2026 ‚Ä¢ üìñ8 min read(1,630 words)\n\nüìÖ **Published:** January 15, 2026‚Ä¢ **Updated:** February 05, 2026\n\nIn a previous post, I wrote about using my VMware lab with an NVIDIA Tesla P4 for running some AI services. However, this deployment was done with the GPU in passthrough mode (I will refer to this a GPU). I wanted to take this to the next level and I also wanted to automate most of the steps. This was for a few reasons, firstly I wanted to get better at automating in general. Secondly, I found the setup brittle and wanted to improve the reliability of deployments. This post will be about using automation to deploying of the VM infrastructure required to be able to run AI Workloads. \n\nI also decided to bite the bullet and update the graphics card I was using to something a bit more modern and capable. After a bit of searching I decided on an [Nvidia A10](https://www.nvidia.com/en-gb/data-center/products/a10-gpu/)\n\nHere is my write-up, and a link to my GitHub [repository](https://github.com/jameskilbynet/iac/tree/main/ansible) with the relevant code.\n\n## Table of Contents\n\n## GPU vs vGPU\n\nFor anyone not familiar, it may be worth giving an overview of the differences between GPU and vGPU. What I describe as ‚ÄúGPU‚Äù is where I am passing the entire Graphics PCI device through to a single VM within vSphere. Using this method has some advantages, firstly, it doesn‚Äôt require any special licences or drivers. The entire PCI card gets passed into the VM as is. However, it has some downsides. The two most important ones for me are that it can only be presented to a single VM at a time and that VM cannot be snapshotted while it is turned on. This made backups convoluted. As I was changing configurations a lot this became tedious. I also wanted to be able to pass the card through to multiple VM‚Äôs\n\nvGPU is only officially supported on ‚Äúdatacentre‚Äù cards from NVIDIA it virtualizes the graphics card and allows you to share it across multiple VM‚Äôs in a similar way to what vSphere has done for compute virtualization for years. It allows you to split the card into some predefined profiles that I have listed below and attach them to multiple virtual machines at the same time.\n\n## Pre req‚Äôs\n\nThere are quite a lot of pre-reqs required to be in place to utilise the attached deployment scripts. So it‚Äôs worth ensuring that all of these are complete.\n\n  * Obviously, you need a vSphere Host and vCentre licensed at least at the enterprise level (I am using Enterprise Plus on 8.0u3)\n  * NVIDIA datacenter Graphics card and associated Host and Guest drivers, I am using an A10 and using driver version 535.247.0 \n  * NVIDIA vGPU licence. Trials are available [here](https://www.nvidia.com/en-gb/data-center/resources/vgpu-evaluation/) if needed\n  * NFS Server (used for NVIDIA software deployment)\n  * Domain hosted with Cloudflare and API [token](http://You can create one at: https://dash.cloudflare.com/profile/api-tokens) with Zone:DNS:Edit permissions.\n  * SSH access to the vGPU VM with SUDO permission\n  * Internal DNS Records created for \n    * vGPU VM\n    * Traefik Dashboard\n    * Test NGINX Server ‚Äì Optional\n    * NFS Server (Used for vGPU Software install)\n\n## Host Preparation\n\nTo make vGPU work successfully, there are two elements required. The first is that the vSphere host has the driver installed. For now, I‚Äôm using the ‚Äò535‚Äô version of the driver, which is the LTS version.` To utilise vGPU with vSphere you need an NVIDIA account to obtain the software. Once this has been obtained, you need to copy the host driver to a VMware datastore. Place the host in maintenance mode and then install the driver.\n    \n    \n    esxcli software vib install -d /vmfs/volumes/02cb3d2b-457ccb84/nvidia/NVIDIA-GRID-vSphere-8.0-535.247.02-535.247.01-539.28.zip\n\nüìã Copy\n\nIf it worked successfully, you should get a result like the below\n    \n    \n    Installation Result\n       Message: Operation finished successfully.\n       VIBs Installed: NVD_bootbank_NVD-VMware_ESXi_8.0.0_Driver_535.247.02-1OEM.800.1.0.20613240\n       VIBs Removed: \n       VIBs Skipped: \n       Reboot Required: false\n       DPU Results: \n\nüìã Copy\n\nI always choose to restart the host after installing the driver. I have been bitten in the past where it says a reboot isn‚Äôt required but it was.\n\nOnce the host has restarted, ssh into it and validate that the driver is talking to the card correctly. This can be done with the nvidia-smi command. This is executed on the ESXi host and will show something similar to the below if its working.\n    \n    \n    +---------------------------------------------------------------------------------------+  \n    | NVIDIA-SMI 535.247.02             Driver Version: 535.247.02   CUDA Version: N/A      |  \n    |-----------------------------------------+----------------------+----------------------+  \n    | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |  \n    | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |  \n    |                                         |                      |               MIG M. |  \n    |=========================================+======================+======================|  \n    |   0  NVIDIA A10                     On  | 00000000:03:00.0 Off |                    0 |  \n    |  0%   41C    P8              22W / 150W |  11200MiB / 23028MiB |      0%      Default |  \n    |                                         |                      |                  N/A |  \n    +-----------------------------------------+----------------------+----------------------+  \n      \n    \n\nüìã Copy\n\n## Guest Setup\n\nI have built some Ansible playbooks that perform a number of activities that make it MUCH easier to get the guest up and running for use with AI workloads and other containers. These have only been tested with Ubuntu but will probably work with other Linux distro‚Äôs without a lot of changes.\n\nAt a high level, they:\n\n  * [Patch Ubuntu ](https://github.com/jameskilbynet/iac/blob/main/ansible/updates/patch_ubuntu.yml)\n  * [Deploy Docker ](https://github.com/jameskilbynet/iac/blob/main/ansible/docker/install_docker.yml)\n  * [Install NVIDIA Guest drivers](https://github.com/jameskilbynet/iac/blob/main/ansible/vGPU/install_nvidia_drivers.yml)\n  * [Install NVIDIA Container Toolkit](http://ansible/vGPU/install_nvidia_containertoolkit.yml)\n  * [Install Traefik ](https://github.com/jameskilbynet/iac/blob/main/ansible/traefik/traefik_deploy.yml)(Optional) but highly recommended\n\n## Ansible Details \n\nI have kept these as separate playbooks for now. This hopefully makes it easier to follow and/or troubleshoot if needed. The playbooks are intended to be run in order.\n\nI am using SemaphoreUI to handle the deployment but this isn‚Äôt required. \n\n### 1\\. Docker deployment\n\nI have covered the deployment of Docker already in [this](https://jameskilby.co.uk/2025/09/managing-my-homelab-with-semaphoreui/ \"Managing my Homelab with SemaphoreUI\") post. Obviously, you don‚Äôt have to use Semaphore to do this (especially as Semaphore requires Docker in the first place). However, you can use the Ansible playbook with some amendments.\n\nNo parameters are required to be set for the Docker deployment. Everything is set in the playbook.\n\n### 2\\. Install NVIDIA Guest Drivers\n\nThis playbook configures an NFS client on the Ubuntu server and then copies both the vGPU license file and the driver from my NFS storage before installing them.\n\nA number of parameters need to be defined or amended to run this successfully in your environment.\n\nThe following parameters need to be set in SemaphoreUI as a variable group. \n\nJump to the full Semaphore Instructions at the bottom\n\nVariable| Default| Description  \n---|---|---  \n`nvidia_vgpu_driver_file`| `NVIDIA-Linux-x86_64-535.247.01-grid.run`| Driver installer filename  \n`nvidia_vgpu_licence_file`| `client_configuration_token_04-08-2025-16-54-19.tok`| License token filename  \n`nvidia_vgpu_driver_path`| `/tmp/<driver_file>`| Local path for installer  \n`nvidia_nfs_server`| `nas.jameskilby.cloud`| NFS server hostname  \n`nvidia_nfs_export_path`| `/mnt/pool1/ISO/nvidia`| NFS export path  \n`nvidia_nfs_mount_point`| `/mnt/iso/nvidia`| Local mount point  \n  \n### 3\\. Install NVIDIA Container Toolkit\n\nThis playbook installs the NVIDIA container toolkit and validates that everything is working by running a Docker container and executing the nvidia-smi command from within a container.\n\nNo parameters are required to be set\n\n### 4\\. Install Traefik\n\nTraefik is a reverse proxy/ingress controller. I am using it effectively as a load balancer in front of my web-based homelab services. I also have it integrated with Let‚Äôs Encrypt and Cloudflare so that it will automatically obtain a trusted certificate for my internal services. This has the added benefit that I don‚Äôt need to remember the relevant port the containers are running on.\n\nThis playbook needs a lot of Variables as can be seen below. In most cases the default is ok.\n\nWhen they are all input to Semaphore it will look something like this.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/Screenshot-2026-02-05-at-23.01.04-806x1024.png)\n\nRather than typing all of the values out you can copy the json below and then just tweak what you need to.\n    \n    \n    {\n      \"nvidia_vgpu_driver_file\": \"NVIDIA-Linux-x86_64-535.247.01-grid.run\",\n      \"nvidia_vgpu_licence_file\": \"client_configuration_token_04-08-2025-16-54-19.tok\",\n      \"nvidia_vgpu_driver_path\": \"/tmp/<driver_file>\",\n      \"nvidia_nfs_server\": \"nas.jameskilby.cloud\",\n      \"nvidia_nfs_export_path\": \"/mnt/pool1/ISO/nvidia\",\n      \"nvidia_nfs_mount_point\": \"/mnt/iso/nvidia\",\n      \"traefik_deploy_test_service\": \"true\",\n      \"traefik_healthcheck_poll_retries\": \"12\",\n      \"traefik_healthcheck_poll_delay\": \"5\",\n      \"traefik_docker_image\": \"traefik:v3.6.4\",\n      \"traefik_name\": \"traefik\",\n      \"traefik_config_path\": \"/etc/traefik\",\n      \"traefik_acme_path\": \"/etc/traefik/acme\",\n      \"traefik_docker_network\": \"traefik\",\n      \"traefik_http_port\": \"80\",\n      \"traefik_https_port\": \"443\",\n      \"acme_dns_delay\": \"10\",\n      \"acme_dns_resolvers\": \"['1.1.1.1:53', '8.8.8.8:53']\",\n      \"traefik_log_level\": \"info\",\n      \"traefik_log_format\": \"json\",\n      \"traefik_log_max_size\": \"10m\",\n      \"traefik_log_max_files\": \"3\",\n      \"traefik_healthcheck_interval\": \"30s\",\n      \"traefik_healthcheck_timeout\": \"10s\",\n      \"traefik_healthcheck_retries\": \"3\",\n      \"traefik_healthcheck_start_period\": \"30s\",\n      \"traefik_test_container\": \"nginx-test\",\n      \"traefik_test_image\": \"nginx:alpine\",\n      \"traefik_test_domain\": \"test.jameskilby.cloud\",\n      \"traefik_dashboard_user\": \"admin\"\n    }\n\nüìã Copy\n\n## Variable Definitions\n\nVariable| Default| Description  \n---|---|---  \n`traefik_deploy_test_service`| ``true``| Set to `false` to skip test nginx deployment  \n`traefik_healthcheck_poll_retries`| `12`| Number of health check poll attempts  \n`traefik_healthcheck_poll_delay`| `5`| Seconds between health check polls  \n`traefik_docker_image`| `traefik:v3`| Traefik Docker image  \n`traefik_name`| `traefik`| Container name  \n`traefik_config_path`| `/etc/traefik`| Config directory  \n`traefik_acme_path`| `/etc/traefik/acme`| ACME/cert directory  \n`traefik_docker_network`| `traefik`| Docker network name  \n`traefik_http_port`| `80`| HTTP port  \n`traefik_https_port`| `443`| HTTPS port  \n`acme_dns_delay`| `10`| Seconds before DNS check  \n`acme_dns_resolvers`| `['1.1.1.1:53', '8.8.8.8:53']`| DNS resolvers  \n`traefik_log_level`| `INFO`| Log level  \n`traefik_log_format`| `json`| Log format  \n`traefik_log_max_size`| `10m`| Max log size  \n`traefik_log_max_files`| `3`| Max log files  \n`traefik_healthcheck_interval`| `30s`| Health check interval  \n`traefik_healthcheck_timeout`| `10s`| Health check timeout  \n`traefik_healthcheck_retries`| `3`| Health check retries  \n`traefik_healthcheck_start_period`| `30s`| Health check grace period  \n`traefik_test_container`| `nginx-test`| Test container name  \n`traefik_test_image`| `nginx:alpine`| Test container image  \n`traefik_test_domain`| `test.<wildcard_domain>`| Test service domain  \n`traefik_dashboard_user`| `admin`| Dashboard username  \n  \nIt also needs additional variables that should be classed as secrets as they are sensitive\n\nThese are the Traefik admin dashboard hash and the Cloudflare API token\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/SemaphoreSecrets-1024x262.png)\n\nTo generate the hash for the password, the easiest way is with Docker\n    \n    \n    docker run --rm httpd:2 htpasswd -nbB admin 'your_password_here'\n\nüìã Copy\n\n## Semaphore Implementation Instructions\n\nAssuming you are going to use SemaphoreUI to execute the playbooks these are the steps you will need to take. If you haven‚Äôt already set it up review my guide [here](https://jameskilby.co.uk/2025/09/managing-my-homelab-with-semaphoreui/)\n\nThe first step is to define a new repository where the playbooks will be executed from. As my Git repo is public no authentication is required. You also need to specify the branch, in this case I am using main.\n\n### Repository\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/SemaphoreRepo-e1769682413872.png)\n\n### Key Store\n\nYou also need to define the authentication from Semaphore to the target workload. This is done in the Key Store section.\n\nI do this in two parts. The first is auth which I choose to do with SSH keys\n\nI then also store the become password securely in Semaphore for use as SUDO\n\n## Inventory\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/Sempahore-Inventory-665x1024.png)\n\n### Task Templates\n\n## Variable Group\n\nWe will need to create a variable group and set the variables when we come to install the NVIDIA drivers\n\nReview the variable table above and set to match your environment. This is what mine looks like\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/Set_Variable_Group-922x1024.png)\n\nThen ensure that the task template is set to use it\n\n![](https://jameskilby.co.uk/wp-content/uploads/2026/01/Variable-Group-1024x547.png)\n\n## üìö Related Posts\n\n  * [Managing my Homelab with SemaphoreUI](https://jameskilby.co.uk/2025/09/managing-my-homelab-with-semaphoreui/)\n\n## Similar Posts\n\n  * [ ![TrueNAS Logo](https://jameskilby.co.uk/wp-content/uploads/2023/05/Screenshot-2023-05-22-at-18.49.21-768x198.png) ](https://jameskilby.co.uk/2023/05/homelab-storage-refresh-part-1/)\n\n[Homelab](https://jameskilby.co.uk/category/homelab/) | [Storage](https://jameskilby.co.uk/category/storage/)\n\n### [Homelab Storage Refresh (Part 1)](https://jameskilby.co.uk/2023/05/homelab-storage-refresh-part-1/)\n\nBy[James](https://jameskilby.co.uk) May 23, 2023October 1, 2025\n\nTable of Contents Background ZFS Overview Read Cache (ARC and L2ARC) ZIL (ZFS Intent Log) Hardware Background I have just completed the move of all my production and media-based storage/services to TrueNAS Scale. ( I will just refer to this as TrueNAS) This is based on my HP Z840 and I have now retired my‚Ä¶\n\n  * [ ![Nvidia Tesla P4 Homelab Setup](https://jameskilby.co.uk/wp-content/uploads/2023/10/IMG_1107-768x403-1.jpg) ](https://jameskilby.co.uk/2023/10/vgpu-setup-in-my-homelab/)\n\n[Homelab](https://jameskilby.co.uk/category/homelab/) | [VMware](https://jameskilby.co.uk/category/vmware/)\n\n### [Nvidia Tesla P4 Homelab Setup](https://jameskilby.co.uk/2023/10/vgpu-setup-in-my-homelab/)\n\nBy[James](https://jameskilby.co.uk) October 23, 2023July 10, 2024\n\nA little while ago I decided to play with vGPU in my homelab. This was something I had dabbled with in the past but never really had the time or need to get working properly. The first thing that I needed was a GPU. I did have a Dell T20 with an iGPU built into‚Ä¶\n\n  * [Homelab](https://jameskilby.co.uk/category/homelab/) | [Networking](https://jameskilby.co.uk/category/networking/)\n\n### [Lab Update ‚Äì Part 3 Network](https://jameskilby.co.uk/2022/01/lab-update-part-3-network/)\n\nBy[James](https://jameskilby.co.uk) January 6, 2022October 1, 2025\n\nI have retired the WatchGuard Devices with the migration to PFSense running bare-metal in one of the Supermicro Nodes. I will likely virtualise this in the future. In terms of network/switching I have moved to an intermediate step here vMotion and Storage are running over DAC‚Äôs while VMware management and VM traffic is still over‚Ä¶\n\n  * [ ](https://jameskilby.co.uk/2022/01/lab-update-part-1-compute/)\n\n[Homelab](https://jameskilby.co.uk/category/homelab/) | [VMware](https://jameskilby.co.uk/category/vmware/)\n\n### [Lab Update ‚Äì Compute](https://jameskilby.co.uk/2022/01/lab-update-part-1-compute/)\n\nBy[James](https://jameskilby.co.uk) January 6, 2022July 10, 2024\n\nQuite a few changes have happened in the lab recently. so I decided to do a multipart blog on the changes. The refresh was triggered by the purchase of a SuperMicro Server (2027TR-H71FRF) chassis with 4x X9DRT Nodes / Blades. This is known as a BigTwin configuration in SuperMicro parlance. This is something I was‚Ä¶\n\n  * [Homelab](https://jameskilby.co.uk/category/homelab/) | [Veeam](https://jameskilby.co.uk/category/veeam/) | [VMware](https://jameskilby.co.uk/category/vmware/)\n\n### [Lab Update ‚Äì Desired Workloads](https://jameskilby.co.uk/2022/01/lab-update-part-5-desired-workloads/)\n\nBy[James](https://jameskilby.co.uk) January 6, 2022November 11, 2023\n\nMy lab is always undergoing change. Partially as I want to try new things or new ways of doing things. Sometimes because I break things (not always by accident) sometimes it‚Äôs a great way to learn‚Ä¶. I decided to list the workloads I am looking to run (some of these are already in place) Infrastucture‚Ä¶\n\n  * [ ![Intel Optane NVMe Homelab](https://jameskilby.co.uk/wp-content/uploads/2023/04/intel_optane_ssd_900p_series_aic_-_right_angle_575px.png) ](https://jameskilby.co.uk/2023/04/intel-optane/)\n\n[Homelab](https://jameskilby.co.uk/category/homelab/) | [Storage](https://jameskilby.co.uk/category/storage/) | [vExpert](https://jameskilby.co.uk/category/vexpert/)\n\n### [Intel Optane NVMe Homelab](https://jameskilby.co.uk/2023/04/intel-optane/)\n\nBy[James](https://jameskilby.co.uk) April 17, 2023October 1, 2025\n\nI have been a VMware vExpert for many years and it has brought me many many benefits over the years. I don‚Äôt think it‚Äôs an understatement to say I probably wouldn‚Äôt have my current role within VMware without it. One of the best benefits has been access to a huge amount of licences for VMware‚Ä¶",
  "excerpt": "![](https://jameskilby.co.uk/wp-content/uploads/2026/01/VMware-NVIDIA-logos_ee2f18dc-615d-4c9e-8f11-9c3c2ce2bf37-prv.png)\n\n[Ansible](https://jameskilby.co.uk/category/ansible/) | [Artificial Intelligence](https://jameskilby.co.uk/category/artificial-intelligence/) | [Containers](https://jameskilby.c..."
}