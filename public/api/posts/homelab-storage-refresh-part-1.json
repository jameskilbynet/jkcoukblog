{
  "slug": "homelab-storage-refresh-part-1",
  "title": "Homelab Storage Refresh (Part 1)",
  "description": "Part 1 of my migration from Synology to TrueNAS scale",
  "date": "2023-05-23T12:07:09Z",
  "modified": "2025-10-01T15:22:13Z",
  "author": "James Kilby",
  "url": "https://jameskilby.co.uk/2023/05/homelab-storage-refresh-part-1/",
  "markdown_url": "/markdown/posts/homelab-storage-refresh-part-1.md",
  "api_url": "/api/posts/homelab-storage-refresh-part-1.json",
  "categories": [
    "Homelab",
    "Storage",
    "Networking",
    "Nutanix",
    "Artificial Intelligence",
    "Docker",
    "VMware",
    "Ansible"
  ],
  "tags": [
    null,
    null,
    null
  ],
  "image": "https://jameskilby.co.uk/wp-content/uploads/2023/05/Screenshot-2023-05-22-at-18.49.21.png",
  "content": "![TrueNAS Logo](https://jameskilby.co.uk/wp-content/uploads/2023/05/Screenshot-2023-05-22-at-18.49.21.png)\n\n[Homelab](https://jameskilby.co.uk/category/homelab/) | [Storage](https://jameskilby.co.uk/category/storage/)\n\n# Homelab Storage Refresh (Part 1)\n\nBy[James](https://jameskilby.co.uk) May 23, 2023October 1, 2025 ‚Ä¢ üìñ9 min read(1,851 words)\n\nüìÖ **Published:** May 23, 2023‚Ä¢ **Updated:** October 01, 2025\n\n## Table of Contents\n\n## Background\n\nI have just completed the move of all my production and media-based storage/services to TrueNAS Scale. ( I will just refer to this as TrueNAS) This is based on my HP Z840 and I have now retired my faithful Synology DS918+ and DX517 expansion bay. (Those have gone to fellow vExperts Paul Wilk and Kev Johnson)\n\nI loved the Synology ecosystem and I still believe it‚Äôs probably the easiest to get up and running and offers a very easy-to-use and performant system. Combine this with a large number of additional apps available together with docker it is very customisable. It makes an excellent entry into homelab storage at a relatively low cost. They are even more suitable for home use when you consider that they are typically quiet and very power efficient. I would have no hesitation in recommending a unit to someone.\n\nHowever, my homelab needs could not be served by the units I had. I was especially constrained by the 2x Gigabit interfaces on the 918+ and the thought of spending multi-thousands for one compatible with my new 25Gb/s network and the number of drives I need really didn‚Äôt appeal. This was compounded by the fact that I already had most of the components I wanted to use. My replacement storage needed to be able to meet my current needs with significantly higher performance capabilities but also have room for future expansion. \n\nTherefore the migration to TrueNAS was undertaken. I had the TrueNAS box running for a while (with a lower spec and only used it occasionally when I needed higher-performance storage)This was due to the significantly higher electrical usage.\n\nHowever, it was time to complete the move of all data and media services. Up until this point, my media stack was running on my Synology DS918+. I have had a few issues recently with the Synology that I suspect are it running out of memory. I had upgraded it to 12GB but this was still not enough. As my Z840 TrueNAS server has way more RAM and CPU capabilities As part of the migration, I also consolidated some additional hardware into the TrueNAS host. This has now become the central piece of kit in my homely running a large number of services.\n\n## ZFS Overview\n\nBefore getting into details of the hardware and software config I think it‚Äôs worth giving an overview of ZFS in case anyone is unfamiliar. SUN Microsystems originally developed ZFS and it is a hugely capable storage platform. It is unusual in that it encompasses a volume manager and a file system. It is legendary for its data integrity capabilities and the scale of the system that could be deployed (it was originally called Zettabyte File System) It is a copy-on-write system (COW) If you are familiar with ZFS operation skip this section.\n\nZFS like a lot of modern storage technologies(VSAN etc) wants exclusive access to the underlying disks. This means using a HBA-based device rather than a RAID card. ZFS then groups these drives into a ‚ÄúPool‚Äù \n\nThe default is to use drives for data storage ‚ÄúData VDEV‚Äù This can also be done in several layouts and understanding how this is done is very important to performance. Conceptually these are similar to traditional RAID levels with RAID10 like mirror and stripe usually being recommended for high-performance systems and RAIDZ or RAIDZ2 are parity-based mechanisms with single and double drive protection similar to RAID5 and RAID6 this is recommended for large amounts of data that are not performance critical. \n\nOne of the additional advantages of ZFS is that drives of different types can be dedicated to additional roles to improve performance and resilience. \n\nThese additional roles are:\n\n  * Metadata \n  * Log\n  * Cache\n  * Spare\n  * Dedupe\n\nI am using the Cache (L2ARC) and Log (ZIL) functionality and to understand these you have to know a little bit more about how ZFS functions under the covers. \n\n### Read Cache (ARC and L2ARC)\n\nARC and L2ARC are mechanisms to improve read performance only.\n\nARC or Adaptive Read Cache. Is a high-speed in-memory cache that stores recently accessed data. L2ARC stands for L2ARC Cache. It is a secondary cache that can be used to store data that is not currently in ARC. L2ARC can be implemented using storage devices, such as SSDs or NVMe drives. \n\nARC and L2ARC work together to improve the performance of ZFS. When an application requests data, ZFS first checks ARC to see if the data is present. If the data is in ARC, it is returned to the application without having to access the underlying storage devices. If the data is not in ARC, ZFS checks L2ARC. If the data is in L2ARC, it is copied to ARC and then returned to the application. If the data is not in L2ARC, it is read from the underlying storage devices and then added to ARC.\n\nOne advantage to how ZFS implements these cache mechanisms is that they do not need to be redundant as the data also exists in the storage pool. This means that if ARC was lost due to a crash/reboot or an L2ARC drive failed the data would be accessed from the underlying pool at a lower performance.\n\nThe use of ARC and L2ARC can significantly improve the performance of ZFS. In some cases, the use of L2ARC can even double the performance of ZFS. However, it is important to note that the use of ARC and L2ARC can also increase the amount of memory that is required to run ZFS.\n\nIn Truenas Scale only half of the available RAM is able to be used as ARC therefore I have 128GB of RAM available for ARC. The hit rate in my system is typically over 95%. One limitation I wish would be removed is that the ARC is a global cache improving all of your storage needs but the L2ARC is assigned to a pool of storage. Therefore if you have multiple pools you need to have (and configure) multiple L2ARC devices. (This can be done at the command line by adding different partitions onto one device but I am attempting to keep my system configured from GUI only)\n\nL2ARC Configuration\n\nFor L2ARC I am using an Intel P3520 2TB NVMe in U2 form factor on a Startech PCI card. This is a perfect drive for L2ARC as is a heavy read-focused NVMe offering 375,000 IOPS and uptown 2700MB/s of read performance but only 26,000 IOPS and up to 1800MB/s of write performance. This isn‚Äôt an issue as by design the L2ARC fills slowly and with the later builds of Truenas, the data persists across reboots. This also helps with the relatively low data endurance of 1095 TBW \n\n### ZIL (ZFS Intent Log)\n\nThe ZIL or LOG device(s) is most likely the most misunderstood mechanism within ZFS. Its intention is to improve the performance of synchronous writes. So we need to know what a synchronous write is.\n\nSo let‚Äôs start with the definition: A synchronous write in ZFS is a write operation that is guaranteed to be committed to stable storage before the write operation returns. This means that the data will be written to the underlying storage devices and will not be lost in the event of a system crash or power failure.\n\nIn contrast, an asynchronous write doesn‚Äôt have to hit persistent storage before it‚Äôs committed this leads to higher performance at the risk of data loss. \n\nWhen running VM‚Äôs sync write is basically essential. \n\nThe ZIL is used in the following way:\n\n  1. When an application (Or VM) writes data to a ZFS file system, the data is first written to the ZIL.\n  2. Once the data has been written to the ZIL, it is then written to the underlying storage devices.\n  3. Once the data has been written to the underlying storage devices, the ZIL entry is marked as complete.\n\nThe ZIL is always present on a ZFS pool and if no dedicated device is used then this uses the existing pool devices. This increases the IO load on the pool devices and therefore for performance reasons, a dedicated high-performance device is highly recommended. Although technically a single device will achieve the higher performance if this device fails or is unavailable then the pool is also toast so at a minimum 2 should be considered. \n\nZIL Config \n\nFor the ZIL I am using two Intel Optane P4800X 750GB devices in a mirror configuration. I was VERY lucky to get these sample hardware devices from Intel as part of the [VMware vExpert](https://vexpert.vmware.com) program and I have some blogs coming up dedicated to these devices.\n\nThese are capable of reading 550000 IOPS (4K Blocks) and unto 2500MB/s sequentially read and 550000 IOPS (4K Blocks) 2200 MB/s write Some crazy numbers‚Ä¶. The other thing that makes Optanes the best choice for this role is the endurance that they offer. The spec sheet says they will handle 41 Petabytes being written to them. \n\n## Hardware\n\nIn terms of the setup of TrueNas server, it‚Äôs installed on an HP Z840. This has 2 x Intel Xeon E5-2673 V3 ‚Äì 12-Core 2.40Ghz (48 Threads) and 256GB of ram. This is a high-end workstation device and sadly I don‚Äôt have the rack mount kit for it. However, it was chosen for its horsepower and ability to run all of the PCIe and storage devices that I needed. This includes having the cooling and electrical capabilities to run all the components. It has an 1125-watt power supply so we should be good. There are currently 19 different drives located within the chassis and it‚Äôs a little tight‚Ä¶ See the PCI slots below.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2023/05/IMG_0397-1024x737.jpeg)\n\nI have mentioned the ARC Device and the Optanes (Slots 1,2 + 4) in the above picture. The last thing to mention is the HBA which is an LSI 9300 located in slot 3. All of my other data drives are connected to this.\n\nThe current disk architecture is shown below. However, this is likely not the final setup and it may change temporarily for testing.\n\nRole| Number| Device| Config| Usable/Role  \n---|---|---|---|---  \nBoot Drive | 2 | Intel 80GB SSD | MIRROR | N/A only used for TrueNas OS  \nSSD Pool | 6 | Samsung 860 EVO 2TB | 2 x MIRROR | 3 wide | 4.71TiB (TrueNas Apps and VM storage)  \nHD Pool 1 | 4 | Seagate IronWolf 8TB  | 1xRAIDZ1 | 21TiB ( Media storage)   \nHD Pool 2 | 4 | HGST 1TB 7200RPM | 1xRAIDZ1 | 3TiB. ( Files and Photos)   \nARC (Assigned to SSD Pool) | 1 | Intel 2TB NVMe | JBOD | N/A ARC is only used as a read cache and doesn‚Äôt contribute to capacity  \nSLOG (Assigned to SSD Pool) | 2 | [Intel DC P4800X Optane](https://ark.intel.com/content/www/us/en/ark/products/97154/intel-optane-ssd-dc-p4800x-series-750gb-2-5in-pcie-x4-3d-xpoint.html) | 2 x MIRROR | SLOG is a write log and doesn‚Äôt add to capacity  \n  \nI will do some proper performance testing in my next post and Im sure there optimisations I can make before I do that but until that‚Äôs done. I did just run CrystalMark to ensure things are running in the right ballpark.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2023/05/Screenshot-2023-05-19-at-10.54.27.png)TrueNAS Scale performance \n\nI dug out some old testing from the Synology. The test isn‚Äôt exactly the same but you can see the Read/Write and especially the IOPS are significantly higher.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2022/01/Synology-1024x725.png)Synology Performance\n\n## üìö Related Posts\n\n  * [Automating the deployment of my Homelab AI Infrastructure](https://jameskilby.co.uk/2026/02/automating-the-deployment-of-my-ai-homelab-and-other-improvements/)\n  * [Managing my Homelab with SemaphoreUI](https://jameskilby.co.uk/2025/09/managing-my-homelab-with-semaphoreui/)\n  * [Warp &#8211; The intelligent terminal](https://jameskilby.co.uk/2025/04/warp-the-intelligent-terminal/)\n\n## Similar Posts\n\n  * [Homelab](https://jameskilby.co.uk/category/homelab/) | [Networking](https://jameskilby.co.uk/category/networking/)\n\n### [Lab Update ‚Äì Part 3 Network](https://jameskilby.co.uk/2022/01/lab-update-part-3-network/)\n\nBy[James](https://jameskilby.co.uk) January 6, 2022October 1, 2025\n\nI have retired the WatchGuard Devices with the migration to PFSense running bare-metal in one of the Supermicro Nodes. I will likely virtualise this in the future. In terms of network/switching I have moved to an intermediate step here vMotion and Storage are running over DAC‚Äôs while VMware management and VM traffic is still over‚Ä¶\n\n  * [ ![Configure DHCP Option 43 for UniFi devices to enable remote adoption across subnets](https://jameskilby.co.uk/wp-content/uploads/2024/06/Ubiquiti_Networks-Logo.wine_-768x512.png) ](https://jameskilby.co.uk/2024/06/unifi-dhcp-option-43/)\n\n[Homelab](https://jameskilby.co.uk/category/homelab/) | [Networking](https://jameskilby.co.uk/category/networking/)\n\n### [Configure DHCP Option 43 for UniFi devices to enable remote adoption across subnets](https://jameskilby.co.uk/2024/06/unifi-dhcp-option-43/)\n\nBy[James](https://jameskilby.co.uk) June 26, 2024January 18, 2026\n\nHow to configure DHCP Option 43 for UniFi devices \n\n  * [ ![Nutanix CE](https://jameskilby.co.uk/wp-content/uploads/2020/07/nutanix-logo-HI-REZ_reverse-w-carrier-768x196.jpg) ](https://jameskilby.co.uk/2018/01/nutanix-ce/)\n\n[Homelab](https://jameskilby.co.uk/category/homelab/) | [Nutanix](https://jameskilby.co.uk/category/nutanix/)\n\n### [Nutanix CE](https://jameskilby.co.uk/2018/01/nutanix-ce/)\n\nBy[James](https://jameskilby.co.uk) January 6, 2018July 10, 2024\n\nI ran a Nutanix CE server at home for a little while when it first came out. However, due to the fairly high requirements, it didn‚Äôt make sense to me to continue running it at home. This was compounded by the fact that I have many clusters to play with at work. These all run my‚Ä¶\n\n  * [ ![Self Hosting AI Stack using vSphere, Docker and NVIDIA GPU](https://jameskilby.co.uk/wp-content/uploads/2024/10/pexels-tara-winstead-8386440-768x512.jpg) ](https://jameskilby.co.uk/2024/10/self-hosting-ai-stack-using-vsphere-docker-and-nvidia-gpu/)\n\n[Artificial Intelligence](https://jameskilby.co.uk/category/artificial-intelligence/) | [Docker](https://jameskilby.co.uk/category/docker/) | [Homelab](https://jameskilby.co.uk/category/homelab/)\n\n### [Self Hosting AI Stack using vSphere, Docker and NVIDIA GPU](https://jameskilby.co.uk/2024/10/self-hosting-ai-stack-using-vsphere-docker-and-nvidia-gpu/)\n\nBy[James](https://jameskilby.co.uk) October 11, 2024October 1, 2025\n\nArtificial intelligence is all the rage at the moment, It‚Äôs getting included in every product announcement from pretty much every vendor under the sun. Nvidia‚Äôs stock price has gone to the moon. So I thought I better get some knowledge and understand some of this. As it‚Äôs a huge field and I wasn‚Äôt exactly sure‚Ä¶\n\n  * [ ](https://jameskilby.co.uk/2022/01/lab-update-part-1-compute/)\n\n[Homelab](https://jameskilby.co.uk/category/homelab/) | [VMware](https://jameskilby.co.uk/category/vmware/)\n\n### [Lab Update ‚Äì Compute](https://jameskilby.co.uk/2022/01/lab-update-part-1-compute/)\n\nBy[James](https://jameskilby.co.uk) January 6, 2022July 10, 2024\n\nQuite a few changes have happened in the lab recently. so I decided to do a multipart blog on the changes. The refresh was triggered by the purchase of a SuperMicro Server (2027TR-H71FRF) chassis with 4x X9DRT Nodes / Blades. This is known as a BigTwin configuration in SuperMicro parlance. This is something I was‚Ä¶\n\n  * [ ![Managing my Homelab with SemaphoreUI](https://jameskilby.co.uk/wp-content/uploads/2025/07/semaphore-768x768.png) ](https://jameskilby.co.uk/2025/09/managing-my-homelab-with-semaphoreui/)\n\n[Ansible](https://jameskilby.co.uk/category/ansible/) | [Homelab](https://jameskilby.co.uk/category/homelab/)\n\n### [Managing my Homelab with SemaphoreUI](https://jameskilby.co.uk/2025/09/managing-my-homelab-with-semaphoreui/)\n\nBy[James](https://jameskilby.co.uk) September 2, 2025February 1, 2026\n\nAn intro on how I use SemaphoreUI to manage my Homelab",
  "excerpt": "![TrueNAS Logo](https://jameskilby.co.uk/wp-content/uploads/2023/05/Screenshot-2023-05-22-at-18.49.21.png)\n\n[Homelab](https://jameskilby.co.uk/category/homelab/) | [Storage](https://jameskilby.co.uk/category/storage/)\n\n# Homelab Storage Refresh (Part 1)\n\nBy[James](https://jameskilby.co.uk) May 23, 2..."
}