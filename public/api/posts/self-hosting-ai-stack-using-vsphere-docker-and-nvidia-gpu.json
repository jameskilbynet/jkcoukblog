{
  "slug": "self-hosting-ai-stack-using-vsphere-docker-and-nvidia-gpu",
  "title": "Self Hosting AI Stack using vSphere, Docker and NVIDIA GPU",
  "description": "Self Hosting AI Stack using vSphere, Docker and NVIDIA GPU, Self Hosted solutions for AI enthusiasts. Start your AI journey today!",
  "date": "2024-10-11T15:03:26Z",
  "modified": "2025-10-01T15:22:12Z",
  "author": "James Kilby",
  "url": "https://jameskilby.co.uk/2024/10/self-hosting-ai-stack-using-vsphere-docker-and-nvidia-gpu/",
  "markdown_url": "/markdown/posts/self-hosting-ai-stack-using-vsphere-docker-and-nvidia-gpu.md",
  "api_url": "/api/posts/self-hosting-ai-stack-using-vsphere-docker-and-nvidia-gpu.json",
  "categories": [
    "Artificial Intelligence",
    "Docker",
    "Homelab",
    "Storage",
    "Synology",
    "Nutanix",
    "Ansible",
    "Containers",
    "Devops",
    "NVIDIA",
    "Traefik",
    "VMware",
    "TrueNAS Scale",
    "vSAN",
    "vSphere",
    "Networking"
  ],
  "tags": [
    null,
    null,
    null
  ],
  "image": "https://jameskilby.co.uk/wp-content/uploads/2024/10/pexels-tara-winstead-8386440-1024x683.jpg",
  "content": "![](https://jameskilby.co.uk/wp-content/uploads/2024/10/pexels-tara-winstead-8386440-scaled.jpg)\n\n[Artificial Intelligence](https://jameskilby.co.uk/category/artificial-intelligence/) | [Docker](https://jameskilby.co.uk/category/docker/) | [Homelab](https://jameskilby.co.uk/category/homelab/)\n\n# Self Hosting AI Stack using vSphere, Docker and NVIDIA GPU\n\nBy[James](https://jameskilby.co.uk) October 11, 2024October 1, 2025 ‚Ä¢ üìñ5 min read(1,070 words)\n\nüìÖ **Published:** October 11, 2024‚Ä¢ **Updated:** October 01, 2025\n\nArtificial intelligence is all the rage at the moment, It‚Äôs getting included in every product announcement from pretty much every vendor under the sun. Nvidia‚Äôs stock price has gone to the moon. So I thought I better get some knowledge and understand some of this.\n\nAs it‚Äôs a huge field and I wasn‚Äôt exactly sure where to start I decided to follow [Tim‚Äôs](https://www.youtube.com/@TechnoTim) excellent [video ](https://www.youtube.com/watch?v=GrLpdfhTwLg)and [guide](https://technotim.live/posts/ai-stack-tutorial/) as to what he has deployed. This blog is a bit more of a reminder for me as what I have done. I wont go into all the details as Tim has done a better job than I will. \n\n## Table of Contents\n\n## Introduction\n\nYes, you can do some AI things with a CPU but the reality at the moment is that it‚Äôs better suited to being executed on a GPU and depending on what you‚Äôre doing a GPU(s) with lots of memory are what you need. I didn‚Äôt fancy spending any money so I thought I would start with my Nvidia P4 and see where I get to.\n\n### Nvidia Tesla P4 Specs\n\nThe Nvidia P4 has 8GB of DDR5 memory which is enough for running some smaller models \n\n### OS Choice\n\nI chose to use one of my vSphere Hosts with a VM running Ubuntu 24.04 VM \n\n### vHardware Spec\n\nIn vSphere, I allocated 8x vCPUs, 20GB of memory, and 256GB of storage to the VM with the P4 passed in directly. This means I didn‚Äôt need any additional NVIDIA licences.\n\nInstall Ubuntu 24.04 ( or use your template) & make sure it‚Äôs patched\n    \n    \n    sudo apt-get update && sudo apt-get upgrade\n\nüìã Copy\n\n## Install NVIDIA Software\n\nThe first step is to install the Nvidia drivers.\n    \n    \n    sudo ubuntu-drivers install \n    \n\nüìã Copy\n\nreboot and then use ‚Äúsudo nvidia-smi‚Äù to validate that the card is now functioning within the VM. The output below shows that the Ubuntu server can see the Tesla P4\n    \n    \n    +-----------------------------------------------------------------------------+\n    | NVIDIA-SMI 470.256.02   Driver Version: 470.256.02   CUDA Version: 11.4     |\n    |-------------------------------+----------------------+----------------------+\n    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n    |                               |                      |               MIG M. |\n    |===============================+======================+======================|\n    |   0  Tesla P4            Off  | 00000000:0B:00.0 Off |                    0 |\n    | N/A   46C    P0    23W /  75W |      0MiB /  7611MiB |      0%      Default |\n    |                               |                      |                  N/A |\n    +-------------------------------+----------------------+----------------------+\n                                                                                   \n    +-----------------------------------------------------------------------------+\n    | Processes:                                                                  |\n    |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n    |        ID   ID                                                   Usage      |\n    |=============================================================================|\n    |  No running processes found                                                 |\n    +-----------------------------------------------------------------------------+\n\nüìã Copy\n\n## Install Docker\n\nThe next step is to install docker as the AI engines will all be running within docker.\n    \n    \n    # Add Docker's official GPG key:\n    sudo apt-get update\n    sudo apt-get install ca-certificates curl gnupg\n    sudo install -m 0755 -d /etc/apt/keyrings\n    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n    sudo chmod a+r /etc/apt/keyrings/docker.gpg\n    \n    # Add the repository to Apt sources:\n    echo \\\n      \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n      \"$(. /etc/os-release && echo \"$VERSION_CODENAME\")\" stable\" | \\\n      sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n    sudo apt-get update\n\nüìã Copy\n    \n    \n    sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n\nüìã Copy\n\nUse Docker without SUDO\n    \n    \n    sudo usermod -aG docker $USER\n\nüìã Copy\n\n## NVIDIA Container Toolkit\n\nThe next thing to do is to install the Nvidia container toolkit. This allows containers to interact with the graphics card\n    \n    \n    curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n      && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n        sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n        sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n\nüìã Copy\n\nsudo apt-get update\n    \n    \n    sudo apt-get install -y nvidia-container-toolkit\n\nüìã Copy\n    \n    \n    sudo nvidia-ctk runtime configure --runtime=docker\n    sudo systemctl restart docker\n\nüìã Copy\n\nTest NVIDIA Docker\n    \n    \n    sudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n\nüìã Copy\n\nIf the above worked, you have a functioning system where docker can communicate with your GPU.\n\n## Overview\n\nTim‚Äôs blog has a decent overview of the containers and what they are doing so I won‚Äôt detail it here.\n\n## Chat in Action\n\nSo the next obvious thing I needed to do was ask it some questions. So I asked the gemma2 model the question ‚ÄúWhat AI models can I run on a Nvidia p4?‚Äù\n\nand it answered‚Ä¶.\n\nYou‚Äôre right to be specific! While the Nvidia P4 is _technically_ an AI-capable GPU, it‚Äôs designed more for inference (running pre-trained models) than training complex ones from scratch. Think of it like a powerful calculator for AI, not a research lab.\n\nHere‚Äôs a breakdown of what you can realistically expect on an Nvidia P4:\n\n**1\\. Lightweight Models are Your Friend:**\n\n  * **Object Detection:** Tiny YOLOv3 variants or lightweight MobileNet-based detectors. Expect lower accuracy than bigger models but fast inference speeds for real-time applications.\n  * **Image Classification:** MobileNet V2, Inception v3 (downscaled), SqueezeNet ‚Äì these are pre-trained and optimized for mobile devices.\n\n**2\\. Frameworks Matter:**\n\n  * **TensorRT** : This is Nvidia‚Äôs specialized engine for P4. It optimizes your models for performance, even converting them into a more efficient format.\n  * **ONNX Runtime** : This open-source runtime supports various AI frameworks, and can be used to run models on the P4 after conversion.\n\n**3\\. Practical Applications:**\n\n  * **Edge Devices:** Think smart cameras, embedded systems, or robotics where low power consumption and quick responses are crucial.\n  * **Demo Projects:** Great for learning about deployment and seeing how AI works in real-time, even if the accuracy isn‚Äôt state-of-the-art.\n\n**What You CAN‚ÄôT Do:**\n\n  * Train massive neural networks from scratch (think GPT-3 or large image generators). The P4 lacks the memory and compute power.\n  * Run cutting-edge research models that demand immense resources. Those are best left to specialized cloud GPUs or supercomputers.\n\nLet me know if you have a specific AI task in mind ‚Äì I might be able to suggest suitable pre-trained models or optimizations for your Nvidia P4 setup!\n\nAs a response, I was certainly impressed with it\n\n## Power Consumption\n\nObviously, I am very aware of the power usage on my lab so I thought it was interesting to dig into how this much running chat queries may impact things. According to NVTOP The P4 normally idles around 7 Watts, however, once a model is loaded even if its not running any queries this increases to around 24W this is probably due to the increased memory clock speed.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2024/09/Screenshot-2024-09-05-at-11.57.47.png)Idle consumption ![](https://jameskilby.co.uk/wp-content/uploads/2024/09/Screenshot-2024-09-05-at-12.01.26.png)Models loaded ![](https://jameskilby.co.uk/wp-content/uploads/2024/09/Screenshot-2024-09-16-at-14.41.52.png)Models Running\n\nWhen queries are running it will typically use 50-60 watts.\n\n## Summary \n\nObviously it‚Äôs early days for my experimentation into what is a large and rapidly changing field. But it‚Äôs certainly something I will be playing with a lot.\n\n## üìö Related Posts\n\n  * [Automating the deployment of my Homelab AI Infrastructure](https://jameskilby.co.uk/2026/02/automating-the-deployment-of-my-ai-homelab-and-other-improvements/)\n  * [Warp &#8211; The intelligent terminal](https://jameskilby.co.uk/2025/04/warp-the-intelligent-terminal/)\n  * [How I Migrated from Pocket to Hoarder with AI Integration](https://jameskilby.co.uk/2025/01/how-i-migrated-from-pocket-to-hoarder-and-introduced-some-ai-along-the-way/)\n\n## Similar Posts\n\n  * [Homelab](https://jameskilby.co.uk/category/homelab/) | [Storage](https://jameskilby.co.uk/category/storage/) | [Synology](https://jameskilby.co.uk/category/synology/)\n\n### [Lab Storage](https://jameskilby.co.uk/2018/01/lab-storage/)\n\nBy[James](https://jameskilby.co.uk) January 6, 2018July 10, 2024\n\nI have been meaning to post around some of the lab setup for a while. Although it changes frequently at present it‚Äôs as below. I will add some pics when I have tidied up the lab/cables My primary lab storage is all contained within an HP Gen8 Microserver. Currently Configured: 1x INTEL Core i3-4130 running at‚Ä¶\n\n  * [ ![Nutanix CE](https://jameskilby.co.uk/wp-content/uploads/2020/07/nutanix-logo-HI-REZ_reverse-w-carrier-768x196.jpg) ](https://jameskilby.co.uk/2018/01/nutanix-ce/)\n\n[Homelab](https://jameskilby.co.uk/category/homelab/) | [Nutanix](https://jameskilby.co.uk/category/nutanix/)\n\n### [Nutanix CE](https://jameskilby.co.uk/2018/01/nutanix-ce/)\n\nBy[James](https://jameskilby.co.uk) January 6, 2018July 10, 2024\n\nI ran a Nutanix CE server at home for a little while when it first came out. However, due to the fairly high requirements, it didn‚Äôt make sense to me to continue running it at home. This was compounded by the fact that I have many clusters to play with at work. These all run my‚Ä¶\n\n  * [ ![Automating the deployment of my Homelab AI  Infrastructure](https://jameskilby.co.uk/wp-content/uploads/2026/01/VMware-NVIDIA-logos_ee2f18dc-615d-4c9e-8f11-9c3c2ce2bf37-prv-768x432.png) ](https://jameskilby.co.uk/2026/02/automating-the-deployment-of-my-ai-homelab-and-other-improvements/)\n\n[Ansible](https://jameskilby.co.uk/category/ansible/) | [Artificial Intelligence](https://jameskilby.co.uk/category/artificial-intelligence/) | [Containers](https://jameskilby.co.uk/category/containers/) | [Devops](https://jameskilby.co.uk/category/devops/) | [Homelab](https://jameskilby.co.uk/category/homelab/) | [NVIDIA](https://jameskilby.co.uk/category/nvidia/) | [Traefik](https://jameskilby.co.uk/category/traefik/) | [VMware](https://jameskilby.co.uk/category/vmware/)\n\n### [Automating the deployment of my Homelab AI Infrastructure](https://jameskilby.co.uk/2026/02/automating-the-deployment-of-my-ai-homelab-and-other-improvements/)\n\nBy[James](https://jameskilby.co.uk) February 9, 2026February 9, 2026\n\nLearn how to use Ansible to configure an Ubuntu VM for use with NVIDIA based AI workloads in vSphere\n\n  * [ ![How to Run ZFS on VMware vSphere: Setup Guide and Best Practices](https://jameskilby.co.uk/wp-content/uploads/2024/12/ZFS.jpg) ](https://jameskilby.co.uk/2024/12/zfs-on-vmware/)\n\n[TrueNAS Scale](https://jameskilby.co.uk/category/truenas-scale/) | [VMware](https://jameskilby.co.uk/category/vmware/) | [vSAN](https://jameskilby.co.uk/category/vmware/vsan-vmware/) | [vSphere](https://jameskilby.co.uk/category/vsphere/)\n\n### [How to Run ZFS on VMware vSphere: Setup Guide and Best Practices](https://jameskilby.co.uk/2024/12/zfs-on-vmware/)\n\nBy[James](https://jameskilby.co.uk) December 18, 2024February 9, 2026\n\nZFS on VMware Best Practices\n\n  * [ ](https://jameskilby.co.uk/2022/01/lab-update-part-1-compute/)\n\n[Homelab](https://jameskilby.co.uk/category/homelab/) | [VMware](https://jameskilby.co.uk/category/vmware/)\n\n### [Lab Update ‚Äì Compute](https://jameskilby.co.uk/2022/01/lab-update-part-1-compute/)\n\nBy[James](https://jameskilby.co.uk) January 6, 2022July 10, 2024\n\nQuite a few changes have happened in the lab recently. so I decided to do a multipart blog on the changes. The refresh was triggered by the purchase of a SuperMicro Server (2027TR-H71FRF) chassis with 4x X9DRT Nodes / Blades. This is known as a BigTwin configuration in SuperMicro parlance. This is something I was‚Ä¶\n\n  * [Homelab](https://jameskilby.co.uk/category/homelab/) | [Networking](https://jameskilby.co.uk/category/networking/)\n\n### [Lab Update ‚Äì Part 3 Network](https://jameskilby.co.uk/2022/01/lab-update-part-3-network/)\n\nBy[James](https://jameskilby.co.uk) January 6, 2022October 1, 2025\n\nI have retired the WatchGuard Devices with the migration to PFSense running bare-metal in one of the Supermicro Nodes. I will likely virtualise this in the future. In terms of network/switching I have moved to an intermediate step here vMotion and Storage are running over DAC‚Äôs while VMware management and VM traffic is still over‚Ä¶",
  "excerpt": "![](https://jameskilby.co.uk/wp-content/uploads/2024/10/pexels-tara-winstead-8386440-scaled.jpg)\n\n[Artificial Intelligence](https://jameskilby.co.uk/category/artificial-intelligence/) | [Docker](https://jameskilby.co.uk/category/docker/) | [Homelab](https://jameskilby.co.uk/category/homelab/)\n\n# Sel..."
}