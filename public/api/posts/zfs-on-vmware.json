{
  "slug": "zfs-on-vmware",
  "title": "How to Run ZFS on VMware vSphere: Setup Guide and Best Practices",
  "description": "ZFS,VMware,Best Practices for seamless integration and performance. Learn how to optimize your setup for maximum efficiency and reliability.",
  "date": "2024-12-18T17:47:57Z",
  "modified": "2026-02-09T22:16:17Z",
  "author": "James Kilby",
  "url": "https://jameskilby.co.uk/2024/12/zfs-on-vmware/",
  "markdown_url": "/markdown/posts/zfs-on-vmware.md",
  "api_url": "/api/posts/zfs-on-vmware.json",
  "categories": [
    "TrueNAS Scale",
    "VMware",
    "vSAN",
    "vSphere",
    "Homelab",
    "Networking",
    "VMware Cloud on AWS",
    "AWS",
    "Veeam"
  ],
  "tags": [
    null,
    null,
    null,
    null,
    null
  ],
  "image": "https://jameskilby.co.uk/wp-content/uploads/2024/12/IMG_20140330_210511-1024x845.jpeg",
  "content": "![](https://jameskilby.co.uk/wp-content/uploads/2024/12/ZFS.jpg)\n\n[TrueNAS Scale](https://jameskilby.co.uk/category/truenas-scale/) | [VMware](https://jameskilby.co.uk/category/vmware/) | [vSAN](https://jameskilby.co.uk/category/vmware/vsan-vmware/) | [vSphere](https://jameskilby.co.uk/category/vsphere/)\n\n# How to Run ZFS on VMware vSphere: Setup Guide and Best Practices\n\nBy[James](https://jameskilby.co.uk) December 18, 2024February 9, 2026 ‚Ä¢ üìñ3 min read(544 words)\n\nüìÖ **Published:** December 18, 2024‚Ä¢ **Updated:** February 09, 2026\n\n## Table of Contents\n\n## Introduction\n\nI have run a number of systems using ZFS since the earliest days of my homelab using Nexenta, all the way back in 2010. The image below command is my lab at the time with an IBM head unit that I think had 18GB of RAM 6x450GB SAS drives and this was then connected to the Dell PowerVault SCSI Array above it with 14x146GB 10K SAS drives‚Ä¶.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2024/12/IMG_20140330_210511-1024x845.jpeg)Original Nexenta Setup\n\nThe number one rule is to ALWAYS give ZFS access to the underlying raw storage. You don‚Äôt want a raid controller or anything else interfering with the IO path. This is similar to how vSAN works with VMware.\n\nBut rules are meant to be broken right‚Ä¶.. \n\nI have virtualized a few copies of TrueNAS Scale and Core using ZFS on top of VMware. In these particular instances I specifically DON‚ÄôT want to pass through the storage HBA. Why would I do this? Mainly of two reasons. This allows me to test upgrades of my physical TrueNAS setup with an easy rollback if needed by not passing the drives or controllers in I can clone and snapshot the VMs‚Äôs just as if it was any other and move it around my lab infrastructure.\n\n## Copy on Write\n\nZFS is a ‚ÄúCopy on Write‚Äù file system which means that it never overwrites existing blocks of storage. It always places writes into new blocks. This is unfriendly with ‚Äúthin provisioning‚Äù something I am a huge fan of. This means that over time even a tiny database writing one megabyte file over and over again will slowly clog the entire file system.\n\nSo if you‚Äôre going to break the rules. The way I see it is you might as well do it properly\n\nThe first requirement is that the VM‚Äôs be provisioned with thin disks in vSphere. If is not thin then unmap won‚Äôt work. This is important in case you are thin at the underlay storage level.\n\n## Disk IDs\n\nYou also need to do is to to ensure that TrueNAS can see unique disk IDs. To do this shut down the VM‚Äôs and add the following parameter to the VMware VM‚Äôs configuration\n    \n    \n     disk.EnableUUID=TRUE\n\nüìã Copy\n\nOnce this is done when you power the VM‚Äôs on you should be able to see unique serials of each disk similar to this screenshot. Prior to this change, the serial section is blank.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2024/12/Disk-Serials-1024x125.png)\n\n## Trim\n\nOnce the disks are seen as unique it is possible to enable trim. To confirm that trim is working execute the below command. ( I have no idea why this is a blocker but it is)\n    \n    \n    Sudo pool trim Pool-1\n\nüìã Copy\n\nI decided to manually enable it by executing the command command in the shell. (my Pool is called Pool-1)\n\nTo confirm that To confirm that trim is working execute the below command command is working execute the below command command\n    \n    \n    sudo zpool status Pool-1\n\nüìã Copy\n\nIf everything is working you will see trimming running next to the pool.\n\n![](https://jameskilby.co.uk/wp-content/uploads/2024/12/trimming-1-1024x452.png)\n\nA further validation that this is working is to review the VMs‚Äôs storage used, see the before and after of this VM‚Äôs storage\n\n![](https://jameskilby.co.uk/wp-content/uploads/2024/12/vm-Before-2.png) ![](https://jameskilby.co.uk/wp-content/uploads/2024/12/vm-After.png)\n\nAdditional confirmation can be seen by reviewing the underlying (vSAN consumption in this case). Before and after listed below command\n\n![](https://jameskilby.co.uk/wp-content/uploads/2024/12/vSAN-Before.png) ![](https://jameskilby.co.uk/wp-content/uploads/2024/12/vSAN-After-1.png)\n\n## üìö Related Posts\n\n  * [Can you really squeeze 96TB in 1U ?](https://jameskilby.co.uk/2024/09/can-you-really-squeeze-96tb-in-1u/)\n  * [TrueNAS Scale Useful Commands](https://jameskilby.co.uk/2023/11/truenas-scale-useful-commands/)\n\n## Similar Posts\n\n  * [Homelab](https://jameskilby.co.uk/category/homelab/) | [Networking](https://jameskilby.co.uk/category/networking/)\n\n### [Lab Update ‚Äì Part 3 Network](https://jameskilby.co.uk/2022/01/lab-update-part-3-network/)\n\nBy[James](https://jameskilby.co.uk) January 6, 2022October 1, 2025\n\nI have retired the WatchGuard Devices with the migration to PFSense running bare-metal in one of the Supermicro Nodes. I will likely virtualise this in the future. In terms of network/switching I have moved to an intermediate step here vMotion and Storage are running over DAC‚Äôs while VMware management and VM traffic is still over‚Ä¶\n\n  * [ ![vSAN Cluster Shutdown ‚Äì Orchestration](https://jameskilby.co.uk/wp-content/uploads/2023/11/OrigionalPoweredByvSAN-550x324-1.jpg) ](https://jameskilby.co.uk/2025/12/vsan-cluster-shutdown/)\n\n[VMware](https://jameskilby.co.uk/category/vmware/) | [vSAN](https://jameskilby.co.uk/category/vmware/vsan-vmware/)\n\n### [vSAN Cluster Shutdown ‚Äì Orchestration](https://jameskilby.co.uk/2025/12/vsan-cluster-shutdown/)\n\nBy[James](https://jameskilby.co.uk) December 6, 2025February 1, 2026\n\nHow to safety shutdown a vSAN Environment\n\n  * [ ![VMC Host Errors](https://jameskilby.co.uk/wp-content/uploads/2022/11/iu-1-768x395.png) ](https://jameskilby.co.uk/2020/09/vmc-host-errors/)\n\n[VMware](https://jameskilby.co.uk/category/vmware/) | [VMware Cloud on AWS](https://jameskilby.co.uk/category/vmware/vmware-cloud-on-aws/)\n\n### [VMC Host Errors](https://jameskilby.co.uk/2020/09/vmc-host-errors/)\n\nBy[James](https://jameskilby.co.uk) September 15, 2020February 9, 2026\n\nLean how host failures are handled within VMC\n\n  * [VMware](https://jameskilby.co.uk/category/vmware/) | [AWS](https://jameskilby.co.uk/category/aws/) | [Veeam](https://jameskilby.co.uk/category/veeam/)\n\n### [Monitoring VMC ‚Äì Part 1](https://jameskilby.co.uk/2019/12/monitoring-vmc-part-1/)\n\nBy[James](https://jameskilby.co.uk) December 17, 2019October 1, 2025\n\nAs previously mentioned I have been working a lot with VMware Cloud on AWS and one of the questions that often crops up is around an approach to monitoring. This is an interesting topic as VMC is technically ‚Äúas a service‚Äù therefore the monitoring approach is a bit different. Technically AWS and VMware‚Äôs SRE teams‚Ä¶\n\n  * [ ![VMC New Host -i3en](https://jameskilby.co.uk/wp-content/uploads/2022/11/iu-1-768x395.png) ](https://jameskilby.co.uk/2020/07/i3en/)\n\n[VMware](https://jameskilby.co.uk/category/vmware/) | [VMware Cloud on AWS](https://jameskilby.co.uk/category/vmware/vmware-cloud-on-aws/)\n\n### [VMC New Host -i3en](https://jameskilby.co.uk/2020/07/i3en/)\n\nBy[James](https://jameskilby.co.uk) July 2, 2020July 10, 2024\n\nVMware Cloud on AWS (VMC) has introduced a new host to its lineup the ‚Äúi3en‚Äù. This is based on the i3en.metal AWS instance. The specifications are certainly impressive packing in 96 logical cores, 768GiB RAM, and approximately 45.84 TiB of NVMe raw storage capacity per host. It‚Äôs certainly a monster with a 266% uplift in‚Ä¶\n\n  * [ ](https://jameskilby.co.uk/2022/01/lab-update-part-1-compute/)\n\n[Homelab](https://jameskilby.co.uk/category/homelab/) | [VMware](https://jameskilby.co.uk/category/vmware/)\n\n### [Lab Update ‚Äì Compute](https://jameskilby.co.uk/2022/01/lab-update-part-1-compute/)\n\nBy[James](https://jameskilby.co.uk) January 6, 2022July 10, 2024\n\nQuite a few changes have happened in the lab recently. so I decided to do a multipart blog on the changes. The refresh was triggered by the purchase of a SuperMicro Server (2027TR-H71FRF) chassis with 4x X9DRT Nodes / Blades. This is known as a BigTwin configuration in SuperMicro parlance. This is something I was‚Ä¶",
  "excerpt": "![](https://jameskilby.co.uk/wp-content/uploads/2024/12/ZFS.jpg)\n\n[TrueNAS Scale](https://jameskilby.co.uk/category/truenas-scale/) | [VMware](https://jameskilby.co.uk/category/vmware/) | [vSAN](https://jameskilby.co.uk/category/vmware/vsan-vmware/) | [vSphere](https://jameskilby.co.uk/category/vsph..."
}