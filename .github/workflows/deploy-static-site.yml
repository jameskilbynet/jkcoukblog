name: WordPress to Static Site Deploy

on:
  workflow_dispatch:        # Manual trigger
  repository_dispatch:      # Webhook trigger
    types: [wordpress-update]

# Prevent concurrent runs of this workflow
concurrency:
  group: wordpress-static-site-deploy
  cancel-in-progress: false  # Don't cancel running jobs, wait for completion

jobs:
  spell-check:
    runs-on: self-hosted
    timeout-minutes: 15  # Prevent hanging spell checks
    continue-on-error: true  # Don't fail the workflow if spell check fails
    steps:
    - name: Validate required environment variables
      run: |
        echo "üîç Validating required environment variables..."
        MISSING_VARS=()
        
        # Check for required secrets
        if [ -z "${{ secrets.WP_AUTH_TOKEN }}" ]; then
          MISSING_VARS+=("WP_AUTH_TOKEN")
        fi
        
        if [ -z "${{ secrets.OLLAMA_API_CREDENTIALS }}" ]; then
          MISSING_VARS+=("OLLAMA_API_CREDENTIALS")
        fi
        
        # Report results
        if [ ${#MISSING_VARS[@]} -gt 0 ]; then
          echo "‚ùå Missing required environment variables:"
          for var in "${MISSING_VARS[@]}"; do
            echo "  - $var"
          done
          echo ""
          echo "üìù Please configure these secrets in repository settings:"
          echo "   Settings > Secrets and variables > Actions"
          exit 1
        else
          echo "‚úÖ All required environment variables are set"
          echo "  ‚úì WP_AUTH_TOKEN"
          echo "  ‚úì OLLAMA_API_CREDENTIALS"
        fi
    
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch full history
      
    - name: Restore spell check cache
      uses: actions/cache@v4
      with:
        path: .last_spell_check_timestamp
        key: spell-check-${{ github.sha }}
        restore-keys: |
          spell-check-
      
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
        cache-dependency-path: 'requirements.txt'
        
    - name: Install dependencies
      run: |
        echo "üîß Installing Python dependencies..."
        pip install -r requirements.txt
    
    - name: Run spell check
      env:
        WP_AUTH_TOKEN: ${{ secrets.WP_AUTH_TOKEN }}
        OLLAMA_API_CREDENTIALS: ${{ secrets.OLLAMA_API_CREDENTIALS }}
        OLLAMA_URL: https://ollama.jameskilby.cloud
        OLLAMA_MODEL: llama3.1:8b
      run: |
        echo "üìù Running spell check on modified posts..."
        
        # Read last check timestamp from file (if exists)
        TIMESTAMP_FILE=".last_spell_check_timestamp"
        if [ -f "$TIMESTAMP_FILE" ]; then
          LAST_CHECK=$(cat "$TIMESTAMP_FILE")
          echo "üìÖ Last check: $LAST_CHECK"
          export SINCE_TIMESTAMP="$LAST_CHECK"
        else
          echo "üìÖ No previous check found, checking last 3 posts"
        fi
        
        # Run spell checker (only checks posts modified since SINCE_TIMESTAMP if set)
        if [ -n "$SINCE_TIMESTAMP" ]; then
          python3 scripts/ollama_spell_checker.py || echo "‚ö†Ô∏è  Spelling issues detected (non-blocking)"
        else
          python3 scripts/ollama_spell_checker.py 3 || echo "‚ö†Ô∏è  Spelling issues detected (non-blocking)"
        fi
        
        # Save current timestamp for next run
        date -u +"%Y-%m-%dT%H:%M:%SZ" > "$TIMESTAMP_FILE"
        echo "üíæ Saved timestamp for next check"
        
        # Save spell check summary
        echo "## üìù Spell Check Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **Status:** ‚úÖ Completed" >> $GITHUB_STEP_SUMMARY
        echo "- **Last Check:** $(cat $TIMESTAMP_FILE)" >> $GITHUB_STEP_SUMMARY

  build-and-deploy:
    runs-on: self-hosted  # Changed to self-hosted for Cloudflare Access
    timeout-minutes: 60  # Prevent workflow from running indefinitely
    
    steps:
    - name: Validate required environment variables
      run: |
        echo "üîç Validating required environment variables..."
        MISSING_VARS=()
        MISSING_OPTIONAL=()
        
        # Check for required secrets
        if [ -z "${{ secrets.WP_AUTH_TOKEN }}" ]; then
          MISSING_VARS+=("WP_AUTH_TOKEN")
        fi
        
        # Check for optional secrets (warn but don't fail)
        if [ -z "${{ secrets.SLACK_WEBHOOK_URL }}" ]; then
          MISSING_OPTIONAL+=("SLACK_WEBHOOK_URL")
        fi
        
        if [ -z "${{ secrets.PLAUSIBLE_SHARE_LINK }}" ]; then
          MISSING_OPTIONAL+=("PLAUSIBLE_SHARE_LINK")
        fi
        
        # Report required variables
        if [ ${#MISSING_VARS[@]} -gt 0 ]; then
          echo "‚ùå Missing required environment variables:"
          for var in "${MISSING_VARS[@]}"; do
            echo "  - $var"
          done
          echo ""
          echo "üìù Please configure these secrets in repository settings:"
          echo "   Settings > Secrets and variables > Actions"
          exit 1
        else
          echo "‚úÖ All required environment variables are set"
          echo "  ‚úì WP_AUTH_TOKEN"
        fi
        
        # Report optional variables
        if [ ${#MISSING_OPTIONAL[@]} -gt 0 ]; then
          echo ""
          echo "‚ö†Ô∏è  Optional environment variables not set (non-blocking):"
          for var in "${MISSING_OPTIONAL[@]}"; do
            echo "  - $var"
          done
        fi
    
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch full history for accurate git stats in changelog
      
    - name: Restore build cache
      id: cache-restore
      uses: actions/cache@v4
      with:
        path: |
          .image_optimization_cache
          .last_spell_check_timestamp
        key: build-cache-avif-v2-${{ github.sha }}
        restore-keys: |
          build-cache-avif-v2-
    
    - name: Check cache status
      run: |
        echo "üîç Checking cache restoration status..."
        echo "Cache hit: ${{ steps.cache-restore.outputs.cache-hit }}"
        echo "Cache matched key: ${{ steps.cache-restore.outputs.cache-matched-key }}"
        
        if [ -f ".image_optimization_cache/optimization_cache.json" ]; then
          ENTRIES=$(python3 -c "import json; print(len(json.load(open('.image_optimization_cache/optimization_cache.json'))))" 2>/dev/null || echo "0")
          echo "‚úì Cache file exists with $ENTRIES entries"
        else
          echo "‚ö†Ô∏è  Cache file not found (will be created on first run)"
        fi
      
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
        cache-dependency-path: 'requirements.txt'
        
    - name: Install dependencies
      run: |
        echo "üîß Installing Python dependencies..."
        pip install -r requirements.txt
        echo "‚úÖ Dependencies installed"
        echo "üìã Installed packages:"
        pip list | grep -E "(requests|beautifulsoup4)"
        
    - name: Setup Node.js (for Wrangler)
      uses: actions/setup-node@v4
      with:
        node-version: '18'
    
    - name: Install image optimization tools
      run: |
        echo "üñºÔ∏è  Checking image optimization tools..."
        
        # Check if avifenc is already installed
        if command -v avifenc &> /dev/null; then
          echo "‚úÖ avifenc already installed"
          avifenc --version 2>&1 | head -n 1 || echo "Version info not available"
        else
          echo "üì¶ Installing libavif-bin..."
          sudo apt-get update -qq
          sudo apt-get install -y libavif-bin
          echo "‚úÖ libavif-bin installed"
          avifenc --version 2>&1 | head -n 1 || echo "Version info not available"
        fi
        
        # Check for other optimization tools (optional)
        echo ""
        echo "üìä Image optimization tools status:"
        command -v optipng &> /dev/null && echo "  ‚úÖ optipng" || echo "  ‚ö†Ô∏è  optipng not found (optional)"
        command -v jpegoptim &> /dev/null && echo "  ‚úÖ jpegoptim" || echo "  ‚ö†Ô∏è  jpegoptim not found (optional)"
        command -v cwebp &> /dev/null && echo "  ‚úÖ cwebp" || echo "  ‚ö†Ô∏è  cwebp not found (optional)"
        command -v avifenc &> /dev/null && echo "  ‚úÖ avifenc" || echo "  ‚ùå avifenc not found"
        
        # Verify avifenc works by checking help
        if command -v avifenc &> /dev/null; then
          echo ""
          echo "üß™ Testing avifenc..."
          avifenc --help &> /dev/null && echo "  ‚úÖ avifenc is functional" || echo "  ‚ö†Ô∏è  avifenc may have issues"
        fi
        
    - name: Test runner environment
      if: github.event_name == 'workflow_dispatch'  # Only on manual runs
      run: |
        echo "üîç Current working directory: $(pwd)"
        echo "üìã Files in current directory:"
        ls -la
        echo "üìÑ Looking for key files:"
        ls -la wp_to_static_generator.py || echo "‚ùå File not found"
        echo "üêç Python version and path:"
        python --version
        which python
        echo "üì¶ Checking Python dependencies:"
        python -c "import requests; print(f'‚úÖ requests {requests.__version__}')" || echo "‚ùå requests not found"
        python -c "import bs4; print(f'‚úÖ beautifulsoup4 {bs4.__version__}')" || echo "‚ùå beautifulsoup4 not found"
        
    - name: Generate static site
      timeout-minutes: 30  # Timeout for site generation
      env:
        WP_AUTH_TOKEN: ${{ secrets.WP_AUTH_TOKEN }}
      run: |
        # Ensure we're in the right directory
        echo "üìÅ Working from: $(pwd)"
        if [ ! -f "scripts/wp_to_static_generator.py" ]; then
          echo "‚ùå scripts/wp_to_static_generator.py not found!"
          ls -la scripts/
          exit 1
        fi
        
        # Generate static site
        python scripts/wp_to_static_generator.py ./static-output
        
        # Count generated files for summary
        HTML_COUNT=$(find ./static-output -name "*.html" -type f 2>/dev/null | wc -l | xargs)
        TOTAL_SIZE=$(du -sh ./static-output 2>/dev/null | cut -f1)
        echo "üìä Generated $HTML_COUNT HTML files, total size: $TOTAL_SIZE"
        
        # Save to environment file for use in later steps
        echo "HTML_COUNT=$HTML_COUNT" >> $GITHUB_ENV
        echo "TOTAL_SIZE=$TOTAL_SIZE" >> $GITHUB_ENV
        
        # Replace WordPress images with already-optimized versions from public/
        # Also copy AVIF and WebP versions if they exist
        if [ -d "public/wp-content/uploads" ] && [ -d "./static-output/wp-content/uploads" ]; then
          echo "üìã Checking for already-optimized images..."
          REPLACED=0
          AVIF_COPIED=0
          WEBP_COPIED=0
          
          # For each image in static-output, check if optimized version exists in public/
          find ./static-output/wp-content/uploads -type f \( -name "*.png" -o -name "*.jpg" -o -name "*.jpeg" \) | while read new_img; do
            # Get relative path
            rel_path="${new_img#./static-output/}"
            old_img="public/$rel_path"
            base_name="${old_img%.*}"
            
            # Only copy if optimized version AND AVIF version exist
            # This ensures images without AVIF get re-optimized to create AVIF files
            if [ -f "$old_img" ] && [ -f "${base_name}.avif" ]; then
              new_size=$(stat -f%z "$new_img" 2>/dev/null || stat -c%s "$new_img" 2>/dev/null)
              old_size=$(stat -f%z "$old_img" 2>/dev/null || stat -c%s "$old_img" 2>/dev/null)
              
              # If old is smaller (was optimized), use it instead of new WordPress version
              if [ "$old_size" -lt "$new_size" ]; then
                cp "$old_img" "$new_img"
                REPLACED=$((REPLACED + 1))
                
                # Copy AVIF version (we know it exists)
                cp "${base_name}.avif" "${new_img%.*}.avif"
                AVIF_COPIED=$((AVIF_COPIED + 1))
                
                # Also copy WebP version if it exists
                if [ -f "${base_name}.webp" ]; then
                  cp "${base_name}.webp" "${new_img%.*}.webp"
                  WEBP_COPIED=$((WEBP_COPIED + 1))
                fi
              fi
            fi
          done
          
          echo "‚úÖ Reused $REPLACED optimized images from previous build"
          [ $AVIF_COPIED -gt 0 ] && echo "‚úÖ Copied $AVIF_COPIED AVIF files"
          [ $WEBP_COPIED -gt 0 ] && echo "‚úÖ Copied $WEBP_COPIED WebP files"
        fi
        
        # Add generation summary to GitHub Actions UI
        echo "## üìä Site Generation Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **HTML Pages:** $HTML_COUNT" >> $GITHUB_STEP_SUMMARY
        echo "- **Total Size:** $TOTAL_SIZE" >> $GITHUB_STEP_SUMMARY
        echo "- **Output Directory:** `./static-output`" >> $GITHUB_STEP_SUMMARY
        
    - name: Export to Markdown
      run: |
        echo "üìù Exporting content to markdown..."
        python scripts/markdown_exporter.py \
          ./static-output \
          ./static-output/markdown \
          https://jameskilby.co.uk
        
    - name: Generate Markdown API
      run: |
        echo "üîå Generating markdown API..."
        python scripts/markdown_api.py \
          ./static-output/markdown \
          ./static-output/api
        
    - name: Validate HTML build
      timeout-minutes: 5
      run: |
        echo "üîç Validating generated HTML and assets..."
        
        # Run HTML validation on the generated site
        python3 scripts/validate_html.py ./static-output || {
          echo "‚ùå HTML validation failed!"
          echo "‚ö†Ô∏è  This indicates broken links or missing assets in the generated site."
          exit 1
        }
        
        echo "‚úÖ HTML validation passed!"
        
        # Add validation summary to GitHub Actions
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## ‚úÖ HTML Validation" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **Status:** ‚úÖ Passed" >> $GITHUB_STEP_SUMMARY
        echo "- **Checks:** Broken links, missing assets, HTML structure" >> $GITHUB_STEP_SUMMARY
        
    - name: Content quality validation
      timeout-minutes: 5
      continue-on-error: true  # Non-blocking: warnings won't fail the build
      run: |
        echo "üìã Running content quality checks..."
        
        # Copy to public directory for validation (uses relative paths)
        rm -rf public/
        cp -r ./static-output public/
        
        # Run content validator (non-blocking)
        python3 scripts/content_validator.py || {
          echo "‚ö†Ô∏è  Content validation found issues (non-blocking)"
        }
        
        # Parse validation report and add to summary
        if [ -f "validation-report.json" ]; then
          ERROR_COUNT=$(jq '.summary.errors' validation-report.json)
          WARNING_COUNT=$(jq '.summary.warnings' validation-report.json)
          STATUS=$(jq -r '.summary.status' validation-report.json)
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## üìã Content Quality Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** $([ "$STATUS" = "PASS" ] && echo "‚úÖ Pass" || echo "‚ö†Ô∏è Issues Found")" >> $GITHUB_STEP_SUMMARY
          echo "- **Errors:** $ERROR_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "- **Warnings:** $WARNING_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Show top issues if any
          if [ "$ERROR_COUNT" -gt 0 ]; then
            echo "### ‚ùå Top Errors" >> $GITHUB_STEP_SUMMARY
            jq -r '.errors[:3] | .[] | "- " + .message' validation-report.json >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "$WARNING_COUNT" -gt 0 ]; then
            echo "### ‚ö†Ô∏è Top Warnings" >> $GITHUB_STEP_SUMMARY
            jq -r '.warnings[:3] | .[] | "- " + .message' validation-report.json >> $GITHUB_STEP_SUMMARY
          fi
        else
          echo "‚ö†Ô∏è  No validation report generated" >> $GITHUB_STEP_SUMMARY
        fi
        
    - name: Optimize images
      timeout-minutes: 20  # Timeout for image optimization
      shell: bash
      run: |
        echo "üñºÔ∏è  Starting advanced image optimization with AVIF..."
        
        # Run the Python-based optimizer with parallel processing
        # Uses 4 parallel workers, creates AVIF versions by default
        python3 scripts/optimize_images.py ./static-output --workers 4 --json-output optimization-results.json || {
          echo "‚ö†Ô∏è  Image optimization failed (non-blocking)"
          # Set defaults if optimization fails
          echo "PNG_COUNT=0" >> $GITHUB_ENV
          echo "JPG_COUNT=0" >> $GITHUB_ENV
          echo "OPTIMIZED_COUNT=0" >> $GITHUB_ENV
          echo "SKIPPED_COUNT=0" >> $GITHUB_ENV
          echo "SAVED_MB=0.0" >> $GITHUB_ENV
          exit 0
        }
        
        # Parse JSON results and extract metrics
        if [ -f "optimization-results.json" ]; then
          # Count by format type
          PNG_COUNT=$(jq '[.[] | select(.format_type == "PNG")] | length' optimization-results.json)
          JPG_COUNT=$(jq '[.[] | select(.format_type == "JPEG")] | length' optimization-results.json)
          
          # Count optimized vs cached
          OPTIMIZED_COUNT=$(jq '[.[] | select(.was_cached == false)] | length' optimization-results.json)
          SKIPPED_COUNT=$(jq '[.[] | select(.was_cached == true)] | length' optimization-results.json)
          
          # Calculate total savings in MB
          SAVED_BYTES=$(jq '[.[] | .saved_bytes] | add // 0' optimization-results.json)
          SAVED_MB=$(echo "scale=2; $SAVED_BYTES / 1024 / 1024" | bc)
          
          # Calculate average optimization time for newly optimized images
          # Handle case where all images are cached (length = 0)
          AVG_TIME=$(jq '[.[] | select(.was_cached == false) | .duration_ms] | if length > 0 then (add / length | floor) else 0 end' optimization-results.json)
          
          # Count AVIF and WebP versions created
          AVIF_COUNT=$(jq '[.[] | select(.avif_created == true)] | length' optimization-results.json)
          WEBP_COUNT=$(jq '[.[] | select(.webp_created == true)] | length' optimization-results.json)
          
          echo "üìä Optimization Results:"
          echo "   ‚Ä¢ PNG Images: $PNG_COUNT"
          echo "   ‚Ä¢ JPEG Images: $JPG_COUNT"
          echo "   ‚Ä¢ Newly Optimized: $OPTIMIZED_COUNT"
          echo "   ‚Ä¢ Cached (Skipped): $SKIPPED_COUNT"
          echo "   ‚Ä¢ AVIF Created: $AVIF_COUNT"
          echo "   ‚Ä¢ WebP Created: $WEBP_COUNT"
          echo "   ‚Ä¢ Space Saved: ${SAVED_MB}MB"
          echo "   ‚Ä¢ Avg Time/Image: ${AVG_TIME}ms"
          
          # Save to environment for later steps
          echo "PNG_COUNT=$PNG_COUNT" >> $GITHUB_ENV
          echo "JPG_COUNT=$JPG_COUNT" >> $GITHUB_ENV
          echo "OPTIMIZED_COUNT=$OPTIMIZED_COUNT" >> $GITHUB_ENV
          echo "SKIPPED_COUNT=$SKIPPED_COUNT" >> $GITHUB_ENV
          echo "AVIF_COUNT=$AVIF_COUNT" >> $GITHUB_ENV
          echo "WEBP_COUNT=$WEBP_COUNT" >> $GITHUB_ENV
          echo "SAVED_MB=$SAVED_MB" >> $GITHUB_ENV
          echo "AVG_TIME=$AVG_TIME" >> $GITHUB_ENV
          
          # Add to GitHub Actions summary
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## üñºÔ∏è Image Optimization Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Images:** $((PNG_COUNT + JPG_COUNT))" >> $GITHUB_STEP_SUMMARY
          echo "  - PNG: $PNG_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "  - JPEG: $JPG_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "- **Newly Optimized:** $OPTIMIZED_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "- **Already Optimized (Skipped):** $SKIPPED_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "- **Modern Formats:**" >> $GITHUB_STEP_SUMMARY
          echo "  - AVIF: $AVIF_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "  - WebP: $WEBP_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "- **Space Saved:** ${SAVED_MB} MB" >> $GITHUB_STEP_SUMMARY
          echo "- **Avg Time per Image:** ${AVG_TIME} ms" >> $GITHUB_STEP_SUMMARY
          echo "- **Parallel Workers:** 4" >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ö†Ô∏è  No optimization results found"
          echo "PNG_COUNT=0" >> $GITHUB_ENV
          echo "JPG_COUNT=0" >> $GITHUB_ENV
          echo "OPTIMIZED_COUNT=0" >> $GITHUB_ENV
          echo "SKIPPED_COUNT=0" >> $GITHUB_ENV
          echo "SAVED_MB=0.0" >> $GITHUB_ENV
        fi
      continue-on-error: true
    
    - name: Verify cache was updated
      run: |
        echo "üîç Verifying cache was updated after optimization..."
        
        if [ -f ".image_optimization_cache/optimization_cache.json" ]; then
          ENTRIES=$(python3 -c "import json; print(len(json.load(open('.image_optimization_cache/optimization_cache.json'))))" 2>/dev/null || echo "0")
          SIZE=$(wc -c < ".image_optimization_cache/optimization_cache.json")
          echo "‚úì Cache file exists"
          echo "  Entries: $ENTRIES"
          echo "  Size: $SIZE bytes"
          
          if [ "$ENTRIES" -gt 0 ]; then
            echo "‚úì Cache successfully populated"
            echo "  Sample entries:"
            python3 -c "import json; data=json.load(open('.image_optimization_cache/optimization_cache.json')); import itertools; [print(f'    - {k}') for k in itertools.islice(data.keys(), 3)]" 2>/dev/null || true
          else
            echo "‚ö†Ô∏è  WARNING: Cache file is empty!"
            echo "  This means all images were skipped or optimization failed."
          fi
        else
          echo "‚ùå ERROR: Cache file was not created!"
          echo "  The optimization script may have failed to save the cache."
        fi
    
    - name: Convert images to picture elements
      timeout-minutes: 10
      shell: bash
      run: |
        echo "üñºÔ∏è  Converting img tags to picture elements with AVIF/WebP..."
        
        # Debug: Check if AVIF files exist before conversion
        echo "üîç DEBUG: Checking for AVIF files in ./static-output..."
        AVIF_FILES=$(find ./static-output -name "*.avif" -type f 2>/dev/null | wc -l | xargs)
        echo "   Found $AVIF_FILES AVIF files"
        
        if [ "$AVIF_FILES" -gt 0 ]; then
          echo "   Sample AVIF files:"
          find ./static-output -name "*.avif" -type f 2>/dev/null | head -5 || true
        else
          echo "   ‚ö†Ô∏è  WARNING: No AVIF files found! Picture conversion may skip all images."
        fi
        
        # Debug: Check directory structure
        echo ""
        echo "üîç DEBUG: Directory structure:"
        echo "   Current directory: $(pwd)"
        echo "   Static output exists: $([ -d ./static-output ] && echo 'YES' || echo 'NO')"
        echo "   HTML files in static-output: $(find ./static-output -name '*.html' 2>/dev/null | wc -l | xargs)"
        
        # Run the conversion
        echo ""
        python3 scripts/convert_images_to_picture.py ./static-output || {
          echo "‚ö†Ô∏è  Image to picture conversion failed (non-blocking)"
          echo "PICTURE_CONVERTED=0" >> $GITHUB_ENV
          exit 0
        }
        
        # Debug: Verify picture elements were created
        echo ""
        echo "üîç DEBUG: Checking for picture elements after conversion..."
        PICTURE_COUNT=$(find ./static-output -name "*.html" -exec grep -l "<picture>" {} \; 2>/dev/null | wc -l | xargs)
        echo "   HTML files with <picture> tags: $PICTURE_COUNT"
        
        if [ "$PICTURE_COUNT" -gt 0 ]; then
          echo "   ‚úÖ Picture elements successfully created!"
          echo "   Sample:"
          find ./static-output -name "*.html" -exec grep -l "<picture>" {} \; 2>/dev/null | head -1 | xargs grep -o "<picture>[^<]*<source[^>]*avif[^>]*>" | head -1 || true
        else
          echo "   ‚ö†Ô∏è  WARNING: No picture elements found after conversion!"
        fi
        
        echo "PICTURE_CONVERTED=1" >> $GITHUB_ENV
      continue-on-error: true
    
    - name: Brotli compress static files
      timeout-minutes: 10
      run: |
        echo "üóÅÔ∏è  Compressing static files with Brotli..."
        
        # Run Brotli compression
        python3 scripts/brotli_compress.py ./public || {
          echo "‚ö†Ô∏è  Brotli compression failed (non-blocking)"
          echo "BROTLI_COUNT=0" >> $GITHUB_ENV
          echo "BROTLI_SAVED_MB=0.0" >> $GITHUB_ENV
          exit 0
        }
        
        # Count compressed files
        BROTLI_COUNT=$(find ./public -name "*.br" -type f | wc -l | xargs)
        
        # Calculate total size of .br files
        BROTLI_SIZE=$(find ./public -name "*.br" -type f -exec ls -l {} \; | awk '{sum+=$5} END {print sum}')
        BROTLI_SIZE_MB=$(echo "scale=2; $BROTLI_SIZE / 1024 / 1024" | bc)
        
        # Calculate original size for comparison
        ORIGINAL_SIZE=$(find ./public -type f \( -name "*.html" -o -name "*.css" -o -name "*.js" -o -name "*.json" -o -name "*.xml" -o -name "*.svg" \) -exec ls -l {} \; | awk '{sum+=$5} END {print sum}')
        ORIGINAL_SIZE_MB=$(echo "scale=2; $ORIGINAL_SIZE / 1024 / 1024" | bc)
        
        # Calculate savings
        BROTLI_SAVED=$(echo "$ORIGINAL_SIZE - $BROTLI_SIZE" | bc)
        BROTLI_SAVED_MB=$(echo "scale=2; $BROTLI_SAVED / 1024 / 1024" | bc)
        BROTLI_RATIO=$(echo "scale=1; (1 - $BROTLI_SIZE / $ORIGINAL_SIZE) * 100" | bc)
        
        echo "üìä Brotli Compression Results:"
        echo "   ‚Ä¢ Files compressed: $BROTLI_COUNT"
        echo "   ‚Ä¢ Original size: ${ORIGINAL_SIZE_MB}MB"
        echo "   ‚Ä¢ Compressed size: ${BROTLI_SIZE_MB}MB"
        echo "   ‚Ä¢ Space saved: ${BROTLI_SAVED_MB}MB"
        echo "   ‚Ä¢ Compression ratio: ${BROTLI_RATIO}%"
        
        # Save to environment
        echo "BROTLI_COUNT=$BROTLI_COUNT" >> $GITHUB_ENV
        echo "BROTLI_SAVED_MB=$BROTLI_SAVED_MB" >> $GITHUB_ENV
        echo "BROTLI_RATIO=$BROTLI_RATIO" >> $GITHUB_ENV
        
        # Add to GitHub Actions summary
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## üóúÔ∏è Brotli Compression Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **Files Compressed:** $BROTLI_COUNT" >> $GITHUB_STEP_SUMMARY
        echo "- **Original Size:** ${ORIGINAL_SIZE_MB} MB" >> $GITHUB_STEP_SUMMARY
        echo "- **Compressed Size:** ${BROTLI_SIZE_MB} MB" >> $GITHUB_STEP_SUMMARY
        echo "- **Space Saved:** ${BROTLI_SAVED_MB} MB" >> $GITHUB_STEP_SUMMARY
        echo "- **Compression Ratio:** ${BROTLI_RATIO}%" >> $GITHUB_STEP_SUMMARY
      continue-on-error: true

    - name: Apply SEO fixes
      run: |
        echo "üîß Applying SEO fixes to HTML files..."
        python3 scripts/fix_seo_issues.py ./static-output || {
          echo "‚ö†Ô∏è  SEO fixes failed (non-blocking)"
        }
      continue-on-error: true

    - name: Apply performance optimizations
      run: |
        echo "‚ö° Applying performance optimizations..."
        python3 scripts/enhance_html_performance.py ./static-output || {
          echo "‚ö†Ô∏è  Performance enhancements failed (non-blocking)"
        }
      continue-on-error: true

    - name: Commit and push static site
      timeout-minutes: 10  # Timeout for git operations
      run: |
        # Configure git
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Debug: Show current state
        echo "üìã Initial state:"
        echo "Working directory: $(pwd)"
        echo "Current branch: $(git branch --show-current)"
        echo "Local HEAD: $(git rev-parse HEAD)"
        echo "Remote HEAD: $(git ls-remote origin main | cut -f1)"
        
        # IMPORTANT: Preserve the populated cache before reset
        echo "üíæ Preserving optimization cache before reset..."
        if [ -f ".image_optimization_cache/optimization_cache.json" ]; then
          CACHE_ENTRIES=$(python3 -c "import json; print(len(json.load(open('.image_optimization_cache/optimization_cache.json'))))" 2>/dev/null || echo "0")
          echo "  Cache has $CACHE_ENTRIES entries before reset"
          cp -r .image_optimization_cache /tmp/preserved_cache_$$
          echo "  ‚úì Cache backed up to /tmp/preserved_cache_$$"
        fi
        
        # Ensure we're on the latest version of main
        echo "üîÑ Syncing with remote repository..."
        git fetch origin
        echo "After fetch - Remote HEAD: $(git rev-parse origin/main)"
        git reset --hard origin/main
        echo "After reset - Local HEAD: $(git rev-parse HEAD)"
        
        # Restore the cache after reset
        echo "‚ôªÔ∏è  Restoring optimization cache after reset..."
        if [ -d "/tmp/preserved_cache_$$" ]; then
          cp -r /tmp/preserved_cache_$$/optimization_cache.json .image_optimization_cache/
          RESTORED_ENTRIES=$(python3 -c "import json; print(len(json.load(open('.image_optimization_cache/optimization_cache.json'))))" 2>/dev/null || echo "0")
          echo "  ‚úì Cache restored with $RESTORED_ENTRIES entries"
          rm -rf /tmp/preserved_cache_$$
        fi
        
        # Add static files
        rm -rf public/
        mv ./static-output public/
        
        # Convert absolute URLs to relative for better staging compatibility
        echo "üîÑ Converting URLs for staging compatibility..."
        python scripts/convert_to_staging.py
        
        # Generate changelog page
        echo "üìã Generating changelog page..."
        python scripts/generate_changelog.py || echo "‚ö†Ô∏è  Changelog generation failed (non-blocking)"
        
        # Generate stats page
        echo "üìä Generating stats page..."
        PLAUSIBLE_SHARE_LINK="${{ secrets.PLAUSIBLE_SHARE_LINK }}" python3 scripts/generate_stats_page.py || echo "‚ö†Ô∏è  Stats page generation failed (non-blocking)"
        
        # Submit to IndexNow (notify search engines about all pages)
        echo "üîî Submitting URLs to IndexNow..."
        python3 scripts/submit_indexnow.py ./public || echo "‚ö†Ô∏è  IndexNow submission failed (non-blocking)"
        
    - name: Upload search index to Workers KV
      if: success()
      run: |
        echo "üîç Uploading search index to Workers KV..."
        
        # Check if search index exists
        if [ ! -f "public/search-index.json" ]; then
          echo "‚ö†Ô∏è  Search index not found, skipping KV upload"
          exit 0
        fi
        
        # Install Wrangler if not already available
        if ! command -v wrangler &> /dev/null; then
          echo "Installing Wrangler..."
          npm install -g wrangler
        fi
        
        # Upload to KV (requires CLOUDFLARE_API_TOKEN and KV_SEARCH_INDEX_ID secrets)
        if [ -n "${{ secrets.CLOUDFLARE_API_TOKEN }}" ] && [ -n "${{ secrets.KV_SEARCH_INDEX_ID }}" ]; then
          CURRENT_TIME=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          
          npx wrangler kv:key put \
            --namespace-id="${{ secrets.KV_SEARCH_INDEX_ID }}" \
            --binding=SEARCH_INDEX \
            "current" public/search-index.json \
            --metadata="{\"updated\":\"${CURRENT_TIME}\"}" || {
            echo "‚ö†Ô∏è  Failed to upload search index to KV (non-blocking)"
          }
          
          echo "‚úÖ Search index uploaded to Workers KV"
          echo "   Updated: ${CURRENT_TIME}"
          
          # Add to summary
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## üîç Search Index Updated" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **KV Namespace:** SEARCH_INDEX" >> $GITHUB_STEP_SUMMARY
          echo "- **Updated:** ${CURRENT_TIME}" >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ÑπÔ∏è  Skipping KV upload (CLOUDFLARE_API_TOKEN or KV_SEARCH_INDEX_ID not set)"
          echo "   To enable: Add secrets to repository settings"
        fi
      continue-on-error: true
      env:
        CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
    
    - name: Selective cache purge
      if: success()
      run: |
        echo "üóëÔ∏è Purging changed pages from Workers KV cache..."
        
        # Get list of changed HTML files from this deploy
        git fetch origin
        CHANGED_FILES=$(git diff --name-only HEAD origin/main | grep "^public/.*\.html$" || true)
        
        if [ -z "$CHANGED_FILES" ]; then
          echo "‚ÑπÔ∏è  No HTML files changed, skipping cache purge"
          exit 0
        fi
        
        # Check if cache purge is configured
        if [ -z "${{ secrets.CACHE_PURGE_TOKEN }}" ]; then
          echo "‚ÑπÔ∏è  CACHE_PURGE_TOKEN not set, skipping selective purge"
          echo "   To enable: Add CACHE_PURGE_TOKEN to repository secrets"
          exit 0
        fi
        
        echo "Changed HTML files detected:"
        echo "$CHANGED_FILES" | head -10
        
        PURGED_COUNT=0
        
        # Convert file paths to URL paths and purge from cache
        echo "$CHANGED_FILES" | while read file; do
          # Convert public/2026/01/post/index.html -> /2026/01/post/
          URL_PATH=$(echo "$file" | sed 's|^public||' | sed 's|/index\.html$|/|')
          
          echo "Purging: $URL_PATH"
          
          # Call purge endpoint
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
            -X GET "https://jameskilby.co.uk/.purge?path=${URL_PATH}" \
            -H "X-Purge-Token: ${{ secrets.CACHE_PURGE_TOKEN }}")
          
          if [ "$HTTP_CODE" = "200" ]; then
            PURGED_COUNT=$((PURGED_COUNT + 1))
          else
            echo "  ‚ö†Ô∏è  Failed to purge (HTTP $HTTP_CODE)"
          fi
        done
        
        echo ""
        echo "‚úÖ Purged $PURGED_COUNT URLs from cache"
        
        # Add to summary
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## üóëÔ∏è Cache Purged" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **URLs Purged:** $PURGED_COUNT" >> $GITHUB_STEP_SUMMARY
        echo "- **Method:** Selective (changed files only)" >> $GITHUB_STEP_SUMMARY
      continue-on-error: true
    
    - name: Commit and push static site
      timeout-minutes: 10  # Timeout for git operations
      run: |
        # Configure git
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Debug: Show current state
        echo "üìã Initial state:"
        echo "Working directory: $(pwd)"
        echo "Current branch: $(git branch --show-current)"
        echo "Local HEAD: $(git rev-parse HEAD)"
        echo "Remote HEAD: $(git ls-remote origin main | cut -f1)"
        
        # IMPORTANT: Preserve the populated cache before reset
        echo "üíæ Preserving optimization cache before reset..."
        if [ -f ".image_optimization_cache/optimization_cache.json" ]; then
          CACHE_ENTRIES=$(python3 -c "import json; print(len(json.load(open('.image_optimization_cache/optimization_cache.json'))))" 2>/dev/null || echo "0")
          echo "  Cache has $CACHE_ENTRIES entries before reset"
          cp -r .image_optimization_cache /tmp/preserved_cache_$$
          echo "  ‚úì Cache backed up to /tmp/preserved_cache_$$"
        fi
        
        # Ensure we're on the latest version of main
        echo "üîÑ Syncing with remote repository..."
        git fetch origin
        echo "After fetch - Remote HEAD: $(git rev-parse origin/main)"
        git reset --hard origin/main
        echo "After reset - Local HEAD: $(git rev-parse HEAD)"
        
        # Restore the cache after reset
        echo "‚ôªÔ∏è  Restoring optimization cache after reset..."
        if [ -d "/tmp/preserved_cache_$$" ]; then
          cp -r /tmp/preserved_cache_$$/optimization_cache.json .image_optimization_cache/
          RESTORED_ENTRIES=$(python3 -c "import json; print(len(json.load(open('.image_optimization_cache/optimization_cache.json'))))" 2>/dev/null || echo "0")
          echo "  ‚úì Cache restored with $RESTORED_ENTRIES entries"
          rm -rf /tmp/preserved_cache_$$
        fi
        
        # Add static files
        rm -rf public/
        mv ./static-output public/
        
        # Convert absolute URLs to relative for better staging compatibility
        echo "üîÑ Converting URLs for staging compatibility..."
        python scripts/convert_to_staging.py
        
        # Generate changelog page
        echo "üìã Generating changelog page..."
        python scripts/generate_changelog.py || echo "‚ö†Ô∏è  Changelog generation failed (non-blocking)"
        
        # Generate stats page
        echo "üìä Generating stats page..."
        PLAUSIBLE_SHARE_LINK="${{ secrets.PLAUSIBLE_SHARE_LINK }}" python3 scripts/generate_stats_page.py || echo "‚ö†Ô∏è  Stats page generation failed (non-blocking)"
        
        # Check if there are changes to commit
        # Note: .br files are automatically included in public/
        # Note: IndexNow key file (*.txt in root) must be committed so it's accessible at the site root
        git add public/ .last_spell_check_timestamp .image_optimization_cache/ .indexnow_key *.txt indexnow-submission.json validation-report.json 2>/dev/null || true
        
        if [ -n "$(git diff --cached)" ]; then
          echo "üìù Changes detected, committing..."
          git commit -m "üöÄ Auto-update static site - $(date '+%Y-%m-%d %H:%M:%S')"
          
          # Simple push with built-in retry
          echo "üì§ Pushing to remote..."
          if git push origin main; then
            echo "‚úÖ Push successful"
          else
            echo "‚ö†Ô∏è  Push failed, trying sync and retry..."
            git fetch origin
            
            # Try rebase first
            if git rebase origin/main; then
              echo "üì• Rebase successful, pushing again..."
              git push origin main
            else
              echo "‚ö†Ô∏è  Rebase failed, using force push to resolve conflicts..."
              git rebase --abort 2>/dev/null || true
              
              # Reset to origin, regenerate, and force push
              git reset --hard origin/main
              echo "üîÑ Regenerating static site after reset..."
              rm -rf public/ ./static-output
              python scripts/wp_to_static_generator.py ./static-output
              mv ./static-output public/
              git add public/
              git commit -m "üöÄ Auto-update static site - $(date '+%Y-%m-%d %H:%M:%S') [force-resolved]"
              
              # Force push to resolve any conflicts definitively
              echo "üí™ Force pushing to resolve all conflicts..."
              git push --force-with-lease origin main
            fi
          fi
          
          echo "‚úÖ Static site updated successfully"
          
          # Add deployment summary to GitHub Actions UI
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## üöÄ Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** ‚úÖ Successfully deployed" >> $GITHUB_STEP_SUMMARY
          echo "- **Target:** `public/` directory" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit:** $(git rev-parse --short HEAD)" >> $GITHUB_STEP_SUMMARY
          echo "- **Cloudflare Pages:** Will auto-deploy from repository" >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ÑπÔ∏è  No changes to commit"
          
          # Add no-changes summary
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## ‚ÑπÔ∏è Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** No changes detected" >> $GITHUB_STEP_SUMMARY
          echo "- **Note:** Site is already up to date" >> $GITHUB_STEP_SUMMARY
        fi
    - name: Notify on success
      if: success()
      run: |
        echo "‚úÖ Static site generated and committed successfully!"
        echo "üåê Cloudflare Pages will auto-deploy from the 'public' directory"
        echo "üìÅ Generated files are now in the 'public' directory for Pages deployment"
        
    - name: Notify Slack on Success
      if: success()
      uses: slackapi/slack-github-action@v2.0.0
      with:
        webhook: ${{ secrets.SLACK_WEBHOOK_URL }}
        webhook-type: incoming-webhook
        payload: |
          {
            "channel": "#web",
            "username": "GitHub Actions",
            "icon_emoji": ":github:",
            "attachments": [
              {
                "color": "good",
                "title": "‚úÖ jkcoukblog Build Success",
                "text": "jkcoukblog static site built and pushed successfully!",
                "fields": [
                  {
                    "title": "HTML Pages",
                    "value": "${{ env.HTML_COUNT }}",
                    "short": true
                  },
                  {
                    "title": "Site Size",
                    "value": "${{ env.TOTAL_SIZE }}",
                    "short": true
                  },
                  {
                    "title": "Images Optimized",
                    "value": "${{ env.OPTIMIZED_COUNT }} (skipped: ${{ env.SKIPPED_COUNT }})",
                    "short": true
                  },
                  {
                    "title": "Space Saved",
                    "value": "${{ env.SAVED_MB }}MB",
                    "short": true
                  },
                  {
                    "title": "Trigger",
                    "value": "${{ github.event_name == 'workflow_dispatch' && format('Manual by {0}', github.actor) || format('Commit by {0}', github.event.head_commit.author.name || github.actor) }}",
                    "short": true
                  },
                  {
                    "title": "Message",
                    "value": "${{ github.event.head_commit.message || 'Manual workflow dispatch' }}",
                    "short": false
                  }
                ],
                "footer": "üåê Cloudflare Pages will now deploy the site automatically",
                "footer_icon": "https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png",
                "actions": [
                  {
                    "type": "button",
                    "text": "View Workflow",
                    "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                  }
                ]
              }
            ]
          }
        
    - name: Notify on failure
      if: failure()
      run: |
        echo "‚ùå Static site generation or commit failed!"
        
    - name: Notify Slack on Failure  
      if: failure()
      uses: slackapi/slack-github-action@v2.0.0
      with:
        webhook: ${{ secrets.SLACK_WEBHOOK_URL }}
        webhook-type: incoming-webhook
        payload: |
          {
            "channel": "#web",
            "username": "GitHub Actions",
            "icon_emoji": ":github:",
            "attachments": [
              {
                "color": "danger",
                "title": "‚ùå jkcoukblog Build Failed",
                "text": "jkcoukblog deployment failed!",
                "fields": [
                  {
                    "title": "Trigger",
                    "value": "${{ github.event_name == 'workflow_dispatch' && format('Manual by {0}', github.actor) || format('Commit by {0}', github.event.head_commit.author.name || github.actor) }}",
                    "short": true
                  },
                  {
                    "title": "Branch",
                    "value": "${{ github.ref_name }}",
                    "short": true
                  },
                  {
                    "title": "Message",
                    "value": "${{ github.event.head_commit.message || 'Manual workflow dispatch' }}",
                    "short": false
                  }
                ],
                "footer": "Please check the GitHub Actions logs for details",
                "footer_icon": "https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png",
                "actions": [
                  {
                    "type": "button",
                    "text": "View Workflow Logs",
                    "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                  }
                ]
              }
            ]
          }
